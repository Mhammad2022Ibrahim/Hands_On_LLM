{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLtkLBX3rc-I"
   },
   "source": [
    "# Understanding LoRA\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into the model’s layers. Instead of training all model parameters during fine-tuning, LoRA decomposes the weight updates into smaller matrices through low-rank decomposition, significantly reducing the number of trainable parameters while maintaining model performance. For example, when applied to GPT-3 175B, LoRA reduced trainable parameters by 10,000x and GPU memory requirements by 3x compared to full fine-tuning.  \n",
    "LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on attention weights. During inference, these adapter weights can be merged with the base model, resulting in no additional latency overhead. LoRA is particularly useful for adapting large language models to specific tasks or domains while keeping resource requirements manageable.  \n",
    "\n",
    "Key advantages of LoRA\n",
    "1. Memory Efficiency:\n",
    "  * Only adapter parameters are stored in GPU memory\n",
    "  * Base model weights remain frozen and can be loaded in lower precision\n",
    "  * Enables fine-tuning of large models on consumer GPUs\n",
    "\n",
    "2. Training Features:\n",
    "  * Native PEFT/LoRA integration with minimal setup\n",
    "  * Support for QLoRA (Quantized LoRA) for even better memory efficiency\n",
    "\n",
    "3. Adapter Management:\n",
    "  * Adapter weight saving during checkpoints\n",
    "  * Features to merge adapters back into base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLjBf7iKslA5"
   },
   "source": [
    "# Loading LoRA Adapters with PEFT(Parameter-Efficient Fine-Tuning).\n",
    "PEFT is a library that provides a unified interface for loading and managing PEFT methods, including LoRA. It allows you to easily load and switch between different PEFT methods, making it easier to experiment with different fine-tuning techniques.\n",
    "\n",
    "Adapters can be loaded onto a pretrained model with **load_adapter()**, which is useful for trying out different adapters whose weights aren’t merged. Set the active adapter weights with the **set_adapter()** function. To return the base model, you could use **unload()** to unload all of the LoRA modules. This makes it easy to switch between different task-specific weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld1GugFFs5UR"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5Y2oZdEtYAb"
   },
   "source": [
    "# Fine-tune LLM using trl and the SFTTrainer with LoRA\n",
    "The SFTTrainer from trl provides integration with LoRA adapters through the PEFT library. This means that we can fine-tune a model in the same way as we did with SFT, but use LoRA to reduce the number of parameters we need to train.\n",
    "\n",
    "We’ll use the LoRAConfig class from PEFT in our example. The setup requires just a few configuration steps:\n",
    "\n",
    "1. Define the LoRA configuration (rank, alpha, dropout)\n",
    "2. Create the SFTTrainer with PEFT config\n",
    "3. Train and save the adapter weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poE8GGV-vWip"
   },
   "source": [
    "# Using TRL with PEFT\n",
    "PEFT methods can be combined with TRL for fine-tuning to reduce memory requirements. We can pass the LoraConfig to the model when loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1Wp5Cj5vZLB"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Bkb8yMnvvK4"
   },
   "source": [
    "Above, we used **device_map=\"auto\"** to automatically assign the model to the correct device. You can also manually assign the model to a specific device using **device_map={\"\": device_index}**.\n",
    "\n",
    "We will also need to define the **SFTTrainer** with the LoRA configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCvJfnqiwFz4"
   },
   "outputs": [],
   "source": [
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    max_seq_length=max_seq_length,  # Maximum sequence length\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1oLy1tiwdiB"
   },
   "source": [
    "> ✏️ Try it out! Build on your fine-tuned model from the previous section, but fine-tune it with LoRA. Use the HuggingFaceTB/smoltalk dataset to fine-tune a deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model, using the LoRA configuration we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoTaWOZrwurY"
   },
   "source": [
    "# Merging LoRA Adapters\n",
    "After training with LoRA, you might want to merge the adapter weights back into the base model for easier deployment. This creates a single model with the combined weights, eliminating the need to load adapters separately during inference.\n",
    "\n",
    "The merging process requires attention to memory management and precision. Since you’ll need to load both the base model and adapter weights simultaneously, ensure sufficient GPU/CPU memory is available. Using device_map=\"auto\" in transformers will find the correct device for the model based on your hardware.\n",
    "\n",
    "Maintain consistent precision (e.g., float16) throughout the process, matching the precision used during training and saving the merged model in the same format for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJIGzA6jw0Dg"
   },
   "source": [
    "# Merging Implementation\n",
    "After training a LoRA adapter, you can merge the adapter weights back into the base model. Here’s how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opqheS-Kw6Nh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"base_model_name\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Load the PEFT model with adapter\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model, \"path/to/adapter\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 3. Merge adapter weights with base model\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYW2nHVLxA0Z"
   },
   "source": [
    "If you encounter size discrepancies in the saved model, ensure you’re also saving the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8QSYxfuxCSg"
   },
   "outputs": [],
   "source": [
    "# Save both model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"base_model_name\")\n",
    "merged_model.save_pretrained(\"path/to/save/merged_model\")\n",
    "tokenizer.save_pretrained(\"path/to/save/merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjRFVjTRxQ6Q"
   },
   "source": [
    ">✏️ Try it out! Merge the adapter weights back into the base model. Use the HuggingFaceTB/smoltalk dataset to fine-tune a deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model, using the LoRA configuration we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# How to Fine-Tune LLMs with LoRA Adapters using Hugging Face TRL\n",
    "\n",
    "This notebook demonstrates how to efficiently fine-tune large language models using LoRA (Low-Rank Adaptation) adapters. LoRA is a parameter-efficient fine-tuning technique that:\n",
    "- Freezes the pre-trained model weights\n",
    "- Adds small trainable rank decomposition matrices to attention layers\n",
    "- Typically reduces trainable parameters by ~90%\n",
    "- Maintains model performance while being memory efficient\n",
    "\n",
    "We'll cover:\n",
    "1. Setup development environment and LoRA configuration\n",
    "2. Create and prepare the dataset for adapter training\n",
    "3. Fine-tune using `trl` and `SFTTrainer` with LoRA adapters\n",
    "4. Test the model and merge adapters (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pytorch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "56066234836e45ae97542b3d1c85e372",
      "6b9617f44a72477680646df8ac54de47",
      "79c3f1d0e3464a7385903e79971eb2db",
      "11dcd606605d42659ac03ea01b46d41c",
      "c596ba4f044d4c79824d012c63538c8c",
      "56d65371696346009fe193657bc1d6ef",
      "6745a3eefd7544f096f367cb87f96975",
      "c9446a8a92ec478ebacd2f881881582d",
      "c57822e021a3488b9725be907e298fac",
      "3f9c02e5448a4a8989baed1f99bababb",
      "8c3be682d0444e9ba2d80080049a9773",
      "6eee62327a4c41418c2c190baedb0558",
      "d3c588bbe4c6418d9e0dac287dd3ad2f",
      "2901bff05d104e81a9a61de22bd90b21",
      "0c84dee791fc44d9a9b311e65ab2f5b6",
      "740249b6b83f48b3b8ece5fbbf565bbd",
      "7d7670e43fc04881a97ec1f9d939275a",
      "a2a9a58bec934e2c9747c985a61942d4",
      "4ca7faff7ab04ee69626d6b0151fed7f",
      "79414d402a0d40bda15af16c915b1356"
     ]
    },
    "id": "tKvGVxImouLi",
    "outputId": "83a46123-f3b4-48fe-99ff-1977491789f3"
   },
   "outputs": [],
   "source": [
    "# Install the requirements in Google Colab\n",
    "!pip -q install transformers datasets trl huggingface_hub peft\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "9b5038037c0f4c4d8479267461f95447",
      "7b43afd7838c4fe0bb6ab13423f17c40",
      "9b405ccfeb284433a65bd6897e30b1b6",
      "465d6c47bcfd486eabb6dc55d157c062",
      "79d27f2931524a70a1c7c99662562b00",
      "865cdcb54bc84905a4ca528195f222aa",
      "7991be0628694c719ae5f6f375fca909",
      "5d784c65746d4f59b33e225ffce5c8d2",
      "2ea1e2a2b010456482a0296754a2df9f",
      "3523bb34e1ba412bae2d5908231e65ba",
      "abd7e76b059140099d14bf28fe5a56cd",
      "4d48cf653bcf44efafa0a3216cfefb7e",
      "95392fcf3c9b4c879d817ea1ff80380a",
      "d725b8844dce48e2bb3f0057ff34919e",
      "4b8efa8102c74b2c82fd9cc676baf10a",
      "a03d947777e64dd3bac64f1dd4d06171",
      "0b764e113443484f9d21d8303f789161",
      "3551135196444304838e8d254bd82b52",
      "165a43d344cd4c34b25ed912d6fcdd4a",
      "f537dca9530743c08f15436dddeb37a0",
      "41863592fb574dd696ee30c435d7858e",
      "fcf2ab85fa58487d9e6266727cfee279",
      "960a47341a054a4fb1d263b59965e56f",
      "e42ecc0e86c546d5a0af79bfdf349ce2",
      "5c00b2718f214aa6929f12e5f13b0e0e",
      "d25f42d648a64b04a76254959eaada02",
      "2f6f4534ff5947e2b78852d1a326f10b",
      "2fb15bba19c34e10a0699e5831a3e4d5",
      "7b68cbbaf5dc4646ad326c63b586a139",
      "9f649b21f1eb47f6a021aa208c194160",
      "57f308cab609486b862c5afe4e61de43",
      "c4ab06ccb1bb48f69d495011558aeaee",
      "f16b16015baf4453a1bfc616ab8e0624",
      "90ea34fdf57a469689c16d9c7125e2fb",
      "13d995fd5a484561b7c967dcfc3e88b4",
      "db82550aeab04fb2b2f95065711ff385",
      "764897628a9d47a08babaec00f2c3c41",
      "6b126e6a9d9b442ba38724123c509045",
      "9b4330b0b078409f81fff854ffb42d4f",
      "65b4b68e6a1d436b92a52263d80ca095",
      "753420151685435ba97620f8e1f0224d",
      "2054b459efac4f52b4e27639d0d6ac08",
      "9e1c68ff31c64064aacf2fc85a150feb",
      "3eebb1cc5b064ff99c1ce482d7a090bf",
      "2e926b6d552e474b941831a447b2155f",
      "a197095fcd88483789ed65497dd2f484",
      "ea48b41530bd40b5940be7ab339b183f",
      "f2db9b9d57674eb3a86aec56e61afffa",
      "1f59239f7df34ff98103d1ec15d74054",
      "5736e7fc14864e40b21b3a49cc3e8e92",
      "b56d8a4cc2bd4c929a0820d73b93f9c6",
      "5fe2f1fbaffb4ca3b9fbfd7f1c0804e6",
      "a1344b56747c4bb8b1f770a6bf970f34",
      "6f2712aaaf9c4f35b421a9bb40c78d55",
      "4d48c81170054b959a575bff8822fd6e"
     ]
    },
    "id": "z4p6Bvo7ouLk",
    "outputId": "453dd6cf-9490-4396-f48b-b1e779d1c930"
   },
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. Fine-tune LLM using `trl` and the `SFTTrainer` with LoRA\n",
    "\n",
    "The [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` provides integration with LoRA adapters through the [PEFT](https://huggingface.co/docs/peft/en/index) library. Key advantages of this setup include:\n",
    "\n",
    "1. **Memory Efficiency**:\n",
    "   - Only adapter parameters are stored in GPU memory\n",
    "   - Base model weights remain frozen and can be loaded in lower precision\n",
    "   - Enables fine-tuning of large models on consumer GPUs\n",
    "\n",
    "2. **Training Features**:\n",
    "   - Native PEFT/LoRA integration with minimal setup\n",
    "   - Support for QLoRA (Quantized LoRA) for even better memory efficiency\n",
    "\n",
    "3. **Adapter Management**:\n",
    "   - Adapter weight saving during checkpoints\n",
    "   - Features to merge adapters back into base model\n",
    "\n",
    "We'll use LoRA in our example, which combines LoRA with 4-bit quantization to further reduce memory usage without sacrificing performance. The setup requires just a few configuration steps:\n",
    "1. Define the LoRA configuration (rank, alpha, dropout)\n",
    "2. Create the SFTTrainer with PEFT config\n",
    "3. Train and save the adapter weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "7f7fcb4431b640348f1a1d79a1de920e",
      "3cc39d4699aa4d92b0a945efa18a4a29",
      "608225ce43a140adb8e64b303d643756",
      "e960a1ed301a4c858a9338d07ba791c4",
      "b5207a2cf8964fbd88588965973d9e11",
      "724e27b601b847b080bf2b49e50ad0a4",
      "ea1dbbfa192042f8ad477a648586d0d3",
      "d6316245c6514f138b95b420cda988ec",
      "e802fcb6d8ad4b62bf87200294443a20",
      "5b5fab79269a45d0bd7802bced231205",
      "e5ac676add2640558e6cc7c5b1d96f21",
      "7ae8341df12e46be9dc111306a1b8878",
      "a7280c063eb64802bb6a3e0aee1875ee",
      "d79e1545571845be931c8067719c0a42",
      "f65dae48766e4207bc7527ffa78fc7bb",
      "1311e6c56262478da3402186d7973bf5",
      "375f240d938a4027b2fd3bef4b36949f",
      "4f6d38b24ffe45e4977be407a0324968",
      "3a8f5897f4fe4395aded8841533694c7",
      "41f3189ede854a3089ef4ad6e834970f",
      "241f9f48a0164719b9ac373887e362fa",
      "b69b640be7604b5e94eaa2ddb019f7e4",
      "3b42df4e2f014a0b9f8e29be9be997eb",
      "d7520fd6c5a949edbcbc3f4529d81c4a",
      "3c13319acd57429eaf737c15ddbda2a6",
      "fab6e409eab143929f14746c0909bfd0",
      "e19d8e22e1124142bf8523de5e65f5f1",
      "fd4f519e00604047b66ec383af0c5253",
      "4c2c3e4edc024b63b262ad2a406f98a5",
      "e667faaade9d4392a1db56a63092ab97",
      "00af437c793845a5a3025dd6cb8d0d4c",
      "e4380628b4bc4d709e6ea4c79566d743",
      "69e32ee6ba134c27a6105ce5eda055dd",
      "61d8c1924ede4025af69f37b93555a82",
      "9185916fa50e4b68bfbfe7a6e6cebe87",
      "a1d971a27e6341a885101b65114839c2",
      "8472077de53b418da65b927346cc0798",
      "34c994ffa8f14299bfc2efa199c28419",
      "3bd36d29dc4c42d18daeacd2743dc777",
      "649f1f55b4324ddeaa3b8ff51523b452",
      "fd183e30bd3440a7a4fba55b806db45d",
      "ae5bbdaf182a4550b4de5a2f91a257b8",
      "38e34ea6e9e14192beadf642bd07d12a",
      "ea45e9094137457da0a63f302266612a",
      "3363775fdacf495a93b543592e7ad4a0",
      "a67980d3739d4a6b8359ad824f10e638",
      "311b2745f3f64ad7b9be7e840d86c1b6",
      "5d465f1085ae44809245a8c79647e477",
      "c1ab4474ba0b4e63b43fb3555ea226ca",
      "e97f5d0be05d4e1b9e5716aad5375f5d",
      "00e41b7e71414e26a66f659416fce1f9",
      "3344dffceb0a4b82a723cfc618c8c88a",
      "5047bdaabc504ca987384fe2937261ff",
      "3a3edfdab06140119d4251641166da71",
      "d73755ff3a71439cb6548928c865f6e2",
      "5a3f67c77f3f4375a0f4ea5c08fe0318",
      "e6ab3956495a4a7a9c5ad4571a89fb87",
      "90880ccbd1a34f4586b67c903106cc8c",
      "bc0b70ebd0ad4a36878b53f995201b53",
      "824e8684d0434548b415e5c54c1fcf47",
      "4ff38b17f1004a509bbaf1fa7b3a62aa",
      "b23c8994a6df4fc19584b8eaa41b76af",
      "6b92f43f66674421a833504068e70df3",
      "7662bf64f8904d33998863b505f31442",
      "56c82b1c35bf4ebf949b7602da7410cb",
      "47f91421d6014e77b18dca68d3fd86e3",
      "f7c1818ebaee4eb98146c13ab7725004",
      "cd7d705ab9024994b34d99d85f8dec74",
      "fa755b5126434a98bd279c66715df8d9",
      "aa71ec86f0094d6481f1fee38f58fb4a",
      "d7b8a2f522d04de0b4617278ce6ca9de",
      "de1c76a8b01942e1bd92d28f285fc224",
      "9722ef76230b41679e2a82b863c7eeb9",
      "1f7b00a6ab5642c7ace32eebfa5d6681",
      "d76dd98dfba241b097d05d22bf9242c3",
      "7219c641e5d94608961fb3d83973c5ed",
      "d4eb17b83c4f42c891cf9012b2c4ccd7",
      "2abc8110cc994f7e91b29862f9d6c2ff",
      "624fd92848de44db8c8d770301fc8cfd",
      "148426fa7ec24638b53e2bd2dca831f6",
      "2578b3596ec34f98aa78a4cf6d3ab013",
      "e7cb3636887745dba2f51ca123cd08d6",
      "2dfad5a836704498973878582f829216",
      "cba4c2fd1a3f4cd991744222e2b2a78e",
      "9dfd0455c4ce4dbdaefff8ed8ccff5aa",
      "fbc6a08f4446493bad12816c83884cb0",
      "6427ef8244ae4432b2bafaf7c89428ab",
      "7f648134ba114739884d2d282d556884"
     ]
    },
    "id": "j6qAjLYXntoz",
    "outputId": "f192f814-97ad-4ae7-bc0b-838bc2630046"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-Lora\"\n",
    "finetune_tags = [\"smol-course-lora\", \"module_1_lora\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "The `SFTTrainer`  supports a native integration with `peft`, which makes it super easy to efficiently tune LLMs using, e.g. LoRA. We only need to create our `LoraConfig` and provide it to the trainer.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Define LoRA parameters for finetuning</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p>\n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>🐢 Use the general parameters for an abitrary finetune</p>\n",
    "    <p>🐕 Adjust the parameters and review in weights & biases.</p>\n",
    "    <p>🦁 Adjust the parameters and show change in inference results.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "Before we can start our training we need to define the hyperparameters (`TrainingArguments`) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "args = SFTConfig(\n",
    "    # Output settings\n",
    "    output_dir=finetune_name,  # Directory to save model checkpoints\n",
    "    # Training duration\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    max_seq_length=1512, # Maximum sequence length\n",
    "    packing=True,  # Enable input packing for efficiency\n",
    "    # Batch size settings\n",
    "    per_device_train_batch_size=2,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,  # Trade compute for memory savings\n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n",
    "    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n",
    "    max_grad_norm=0.3,  # Gradient clipping threshold\n",
    "    # Learning rate schedule\n",
    "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n",
    "    # Logging and saving\n",
    "    logging_steps=10,  # Log metrics every N steps\n",
    "    save_strategy=\"epoch\",  # Save checkpoint every epoch\n",
    "    # Precision settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    # Integration settings\n",
    "    push_to_hub=False,  # Don't push to HuggingFace Hub\n",
    "    report_to=\"none\",  # Disable external logging\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Special tokens handled by template\n",
    "        \"append_concat_token\": False,  # No additional separator needed\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "We now have every building block we need to create our `SFTTrainer` to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "58baaa9d770c4454a52a376be1dad7c7",
      "7b6c7a037a814df996ae8645fcc1a72c",
      "80f342935909402a863d6e181c38d3f8",
      "4f9322d5810148898e0d29d06335cb60",
      "60c8e199817e487fb983cc136aa5256a",
      "cd6fc1f472bd411e98b20cf237765a83",
      "13f3b7d155134c6ea56491035e0cec17",
      "0472d998ee9f445c921afca34bf88e7b",
      "809814ba342a4c3e9c29dba3f1efd251",
      "54905a8176c84e26a21bdbd4a962ef61",
      "0046f26817164f5f885d3c76f3c3a718",
      "9a07b5de717a4236b46cb7cbf4e3d159",
      "e71a53d92b0e4422ae26ed75475b56bc",
      "bc15c191c53a46fd98d5dac20e528497",
      "b60e2bea495a4cf0ac5c3a2899e12f1a",
      "742b67980f8d4538857b13e56a52b3ce",
      "015fe0275f7a48aaa1c5ba6e4cb840a7",
      "436aadc42eb54f219a73bebe4f22818f",
      "043a09eb60ed4a13a611f43e8a05348d",
      "c934e1a286c94eebb20c949b054fa008",
      "655d390b10c34cd6b0fd1876d45b729a",
      "f0b6eea77dc94b89b376f70680b17905"
     ]
    },
    "id": "M00Har2douLl",
    "outputId": "34c6c396-85fb-4011-ef29-72fcaeff890d"
   },
   "outputs": [],
   "source": [
    "# max_seq_length = 1512  # max sequence length for model and packing of the dataset\n",
    "\n",
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    processing_class=tokenizer,\n",
    "    # max_seq_length=max_seq_length,  # Maximum sequence length\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "Tq4nIYqKouLl",
    "outputId": "7619a12a-5e59-4610-e934-151b53baf339"
   },
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "The training with Flash Attention for 3 epochs with a dataset of 15k samples took 4:14:36 on a `g5.2xlarge`. The instance costs `1.21$/h` which brings us to a total cost of only ~`5.3$`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### Merge LoRA Adapter into the Original Model\n",
    "\n",
    "When using LoRA, we only train adapter weights while keeping the base model frozen. During training, we save only these lightweight adapter weights (~2-10MB) rather than a full model copy. However, for deployment, you might want to merge the adapters back into the base model for:\n",
    "\n",
    "1. **Simplified Deployment**: Single model file instead of base model + adapters\n",
    "2. **Inference Speed**: No adapter computation overhead\n",
    "3. **Framework Compatibility**: Better compatibility with serving frameworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QadpHp_Cnto3"
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdc4n_Ornto4"
   },
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Load LoRA Adapter</h2>\n",
    "    <p>Use what you learnt from the ecample note book to load your trained LoRA adapter for inference.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1UhohVdouLl",
    "outputId": "482b0966-670c-4585-d70a-2928dd0c88d1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "Lets test some prompt samples and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-shSmUbvouLl",
    "outputId": "c846bcea-2f65-4b27-e8cf-8b686211783c"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSz3L0KgNYxK"
   },
   "source": [
    "# 🐕 Try out the bigcode/the-stack-smol dataset and finetune a code generation model on a specific subset data/python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "d5727bce39144e918967e2afdd919ea5",
      "423dbd9cf2fc4bb3b4428e90c4c7ae57",
      "db3eaa9a7b4f434bb2750c88bd717c9a",
      "ef8c809b0a484b73ab245a10453e9d1a",
      "4415a641e4bc42c5984f641a21418064",
      "c2456b1dd2554289a6b1c2c1c9fb783a",
      "f7f27d9c0fc4437782032aceb451b191",
      "31aee4fc724c46b3bec6bbac8d76a998",
      "dcb351cd55b54a3d84b26346c092724d",
      "35019f9ecaf24c0c8bc1eddb887dde60",
      "abcd151a28924bdba8656872cd287e27",
      "f2405dda1ca34b8cb34283b112b01792",
      "dac3e814790b421889e2c35f62ee0ff6",
      "c84b5a0aa44d4b499fa8be552ffa2792",
      "019b368f725a4196971e3791e93412b9",
      "5fb1875e19d941b8a2f893f75fa4ac6e",
      "a465e7a1e6094a1c86988cc2d59ff79a",
      "9b73d89b40b74467be30a986b61a6624",
      "b159f72f93ea44488174b381fe5c2b9d",
      "e33e0fdf231a44ba8c2b7cab0caf6435",
      "c127dceded82482eac4cb536b8540125",
      "0942a732e35f4a6b96df18403fe9e1c8",
      "4be3c33432074ef596a62877ea8c4803",
      "3cc0ab89b9e546a0aa315a57cdcc03fb",
      "d46d9ba1bdf74c51b646910a690875bc",
      "4879c4cfbf134b2da2265453573b2f43",
      "f8a6694d0e6747d0bc4e02bbd21c4055",
      "a21a17aade1e49bf89b204f0a17d41f3",
      "42a8624d6e8a431faa212e27e93fff0c",
      "c9d94b40a9674fe88924b29b83d49b6b",
      "d0a06f8e6951454c8593f6dcd01e5f92",
      "6e2a17e460fa43398ba7a0e5222b4efc",
      "aa148a717c33403fb2c65c952e2dd32f",
      "cf4274e52f8a47869d6b64ccd2adef46",
      "0e9e3629fbec4561b3797c8d74e851df",
      "000b27918ace41e0a7c7bb9de1324027",
      "67c76649c6134da0996dd75a4612dec5",
      "a0b22adc75c74c8a901b3e962bea054a",
      "1f89268ca25d425198aa199ab59681df",
      "15155220389e413e87cae74cda47dcd7",
      "22970af084704a418e0388e0ad6fca34",
      "4827e4867b714502b2dad0e0e8b209f0",
      "b4f727e3d93d44b796d9c7c524b8258b",
      "140ee3a7816645dc84f789dcaf007a3b",
      "48f5602bdc704bb0bdfaa09f1d6ddeaf",
      "995fa440e673410c9a8849af1a12f086",
      "45ff570d44894a69ae11603a2dcfae57",
      "a985ca1aaad24f6392e37d9010f6135c",
      "ac3b6a4d33bb4c4bbabc7fee9d86b7b2",
      "9b2b18b385cf4bc9a706b059670eae05",
      "eeb9d588a7cf41bb874931adc49595b0",
      "090e4e6a79de4e2886fa9b0bd5b78e59",
      "b218171060364b03b99a6cbba9003c10",
      "0e5594655943487882ec628216c797d2",
      "45713ab5d511404abb1b834566893926",
      "ffac5217d8784f1c9574f6240ff61858",
      "0ba13c2e8ecd46348624176f2b82948b",
      "4270def8ee6943acbef9a892873a89eb",
      "dba3c51cd87d41508600a51f4caae22d",
      "c91f9f77ba994996bd97cc1c0a4895ef",
      "be700e5144004308a0d44c7b9149430a",
      "4ad18fb0a4344361adc0f0920e8ddb57",
      "26758c8cfb5d4a08a368b89934696042",
      "d5169cbf15be42ea9bbc4f4cdbb377a7",
      "80c9e3a363f64da3a59bce16bbe3cf59",
      "5c28c6ec921247498632521b4749a181",
      "59874aedf4f44124845e48b1216169a8",
      "11b95398aa98434abee8da9b2f1cbad6",
      "1014bb14afbf4210a02a4a32ea42ae55",
      "2326eef9fe0c4c9ba209e5469c63409c",
      "ae34aefdfb564542a16d21b28a0fbec3",
      "d5f51643ea9049e0ae9ee39437fdea07",
      "9a188d6c99634cbd98b18ddad900a4a5",
      "fec667df9ab64e9e93a75e5aef3268d8",
      "75a48fdf44764075adb0fbe0e08bc9e9",
      "e136112a7644460899320f00a8d5a6de",
      "a6b2a6579a75460f91c5a8387e8e4059",
      "63fc94037bad4cb5af5c8363b7f6601d",
      "40f7e6ea3b11465994f2f7fea3f0d307",
      "f2d80630c01c4743890278cf0ff84a85",
      "8c5bb4cff9e94526b0f0fd317e626186",
      "680d56213488482093f9d53dcc6ecedc",
      "23545ff18dc7424dae5dd9d7a3ca6ee3",
      "0fbf9fdf18094f1e9f22024cb820c8db",
      "1bc9ba418bf54d68a45f35dd0ee955a2",
      "7b14a25f38b34e73870653d3c51f8376",
      "8f316e0d0d104a6eb5f83ba773edabfd",
      "2293c6955e3543b1a30a560091f797bf"
     ]
    },
    "id": "r33vRvQL_EVe",
    "outputId": "c7485fcc-baa7-4300-8414-a6fec205d7c3"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "# model.config.attn_implementation = \"flash_attention_2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set up chat format (important for TRL-style fine-tuning)\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-BigCode-Python-Lora\"\n",
    "finetune_tags = [\"smol-course-bigcode-lora\", \"module_2-lora\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "146c74b5b65d4f81b4ca8acc12c265d1",
      "4a6cac9ebe4d426cacb2a2e672e6eaa2",
      "1fda00210f46403c83ffa4ae96fa4a03",
      "ef7c29356b10486aab349f534b7d5f11",
      "bda77e23caf14e88a0e3f6beeacda890",
      "9c31286b6a874f079e68db8576c4f032",
      "b95d1b6241d34c39b7544b4375097516",
      "0446f129386b48f2835167b689becb4a",
      "0efcab865a984e21b6c208c3a08ed01c",
      "6b6fbfeef95648f89e27312a77f09847",
      "372f4e5521454ed8bd7c4915abd2b278",
      "74a78b84ed42432ca302da067a89a5b5",
      "189d3b70840846a2882a0065b5ad4650",
      "3b9a30f2994447d0be3b86c3c17f8d71",
      "bee3d2de36a24e5e9ee93420a6d60a23",
      "4077f63a246f4e70b21411a78e3fbc87",
      "c7fd73f2fde048bf9ed81724dc0b3902",
      "85b0926e68d54ab0a3c869eb883a8ac8",
      "d846fcbf915e47cbbc237f4084617031",
      "8e5d5483676f48c6a1a73df1e4d63267",
      "98ee82e47c4b44b993ca61999b98957d",
      "6c535739ed314e50b7ca7a25289eb532"
     ]
    },
    "id": "pKBRMBQPJNuK",
    "outputId": "b2591aff-5266-42aa-9e1e-57c77efb3300"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset (Python subset only)\n",
    "ds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\")\n",
    "\n",
    "ds = ds[\"train\"]\n",
    "\n",
    "def convert_to_chat(example):\n",
    "    parts = example[\"repository_name\"].split(\"/\")\n",
    "    repo_name = parts[1] if len(parts) == 2 else parts[0]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Generate code for `{repo_name}/{example['path']}`.\"\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": example[\"content\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds = ds.map(convert_to_chat)\n",
    "\n",
    "# 🛠️ Preprocess dataset: use only the `content` column\n",
    "# now it's messages\n",
    "def preprocess(example):\n",
    "    return {\"messages\": example[\"messages\"]}\n",
    "\n",
    "ds = ds.map(preprocess, remove_columns=ds.column_names)\n",
    "\n",
    "# Shuffle and split into train/test manually\n",
    "split = ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "ds = {\n",
    "    \"train\": split[\"train\"],\n",
    "    \"test\": split[\"test\"]\n",
    "}\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyAwaEp3f78i"
   },
   "outputs": [],
   "source": [
    "ds[\"train\"] = ds[\"train\"].select(range(2500))  # for testing setup\n",
    "ds[\"test\"] = ds[\"test\"].select(range(300))  # for testing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8D_LxWFgA7b",
    "outputId": "aa2b8469-e1fa-4e61-ae06-56ec69f7e3ae"
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwVV6aGMN6k7"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_jX193-Ooc7"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "args = SFTConfig(\n",
    "    # Output settings\n",
    "    output_dir=finetune_name,  # Directory to save model checkpoints\n",
    "    # Training duration\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    max_seq_length=1512, # Maximum sequence length\n",
    "    # max_seq_length=8192,  # Important!\n",
    "    # packing=True,         # Only if flash_attention_2 is enabled\n",
    "    # Batch size settings\n",
    "    per_device_train_batch_size=2,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,  # Trade compute for memory savings\n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n",
    "    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n",
    "    max_grad_norm=0.3,  # Gradient clipping threshold\n",
    "    # Learning rate schedule\n",
    "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n",
    "    # Logging and saving\n",
    "    logging_steps=10,  # Log metrics every N steps\n",
    "    # save_strategy=\"epoch\",  # Save checkpoint every epoch\n",
    "    save_steps=500,\n",
    "    # Precision settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    # Integration settings\n",
    "    # push_to_hub=False,  # Don't push to HuggingFace Hub\n",
    "    # report_to=\"none\",  # Disable external logging\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"qlora_lora6_alpha8_dropout5_epoch3\",\n",
    "    # eval_strategy=\"steps\",# Evaluate the model at regular intervals\n",
    "    eval_strategy=\"epoch\",\n",
    "    # eval_steps=500,# Frequency of evaluation\n",
    "    # eval_steps=50,\n",
    "    # dataset_kwargs={\n",
    "    #     \"add_special_tokens\": False,  # Special tokens handled by template\n",
    "    #     \"append_concat_token\": False,  # No additional separator needed\n",
    "    # },\n",
    "    hub_model_id=finetune_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "BypT_V5lPtha",
    "outputId": "80d5ef20-ce67-40fa-a318-2d185fe66dbe"
   },
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "wandb.init(project=\"bigcode-finetune-lora\", name=\"qlora-test-lora6\", config=args.__dict__)\n",
    "\n",
    "wandb.log({\n",
    "    \"lora_r\": rank_dimension,\n",
    "    \"lora_alpha\": lora_alpha,\n",
    "    \"lora_dropout\": lora_dropout,\n",
    "    \"learning_rate\": args.learning_rate,\n",
    "    \"batch_size\": args.per_device_train_batch_size * args.gradient_accumulation_steps,\n",
    "    \"num_train_epochs\": args.num_train_epochs\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252,
     "referenced_widgets": [
      "ae5b559985e44f2c822ef6792dae949c",
      "be6aad782c7b49ffb4b1f371792a2e60",
      "4ad3a3dc5f2f41d09cb640c911e17a04",
      "336f8c2e8add4b5bac939ca794e550a2",
      "8502e679bde44bf8b3b63e47134dc7b7",
      "b93141acae5743d296856b4df3e6dea6",
      "a8fef7f0c35c40d292b2e252eb55f872",
      "874044052a264f9a8bd1ecb57a8476dc",
      "252b3f707bd64529a8f23a6133f88dd4",
      "2f9afa91fd2141c386a2862f869bb0a2",
      "37223f25276c4b1cb3d9bfd0da2cee83",
      "c99b860bab6a41c48b1cb2ec40488588",
      "8d3fea91b8d646fa859b627530e40ac8",
      "bf6757548eb143938e9e79b985ec7fe3",
      "9bb9395015914c57bfe495af70f0849b",
      "e184022222674148a9c732512012963d",
      "3e4d6fc526e347c9aa24254be7cfd364",
      "e7d9d1b8819943239be39ac7ca8fec56",
      "1b6549d1771f42c3afc421cdb691aca3",
      "8c367003bb724e2e91605310ee22b928",
      "f3583d5ba7cf4b668d588aca6dee7d95",
      "c4f1b461a54f4ae8b381d423cee1c050",
      "537ce8f0beae4e6c8ac1dd38d66037de",
      "ea8b97bc9f914e7a8e2c8916515946a4",
      "ecee7c8664574bd8981a250fbd360ad3",
      "c29a4dc302dc46a89d6a5b0d86e2c22e",
      "12cc697c37ec43b99cd27fb0cab43d5d",
      "89b0d825273740318c1f8daa2465d63b",
      "52c72ac7f2e9487ebc5ceb1e81936aab",
      "d4f2b8b8694f496d8a06af6e8a51f9cf",
      "21e58c99b01446aba895db3161499023",
      "e45dd010e5b14e55ba8f0cdefc044871",
      "7151b71dc37d49b1ac0e36f2ad4343e6",
      "08aa90cd1f97460e931e4965938e5ad9",
      "d758d52dfc6b4df4a5992500e4740057",
      "b09b30e9fb6d4c52ae61d0147aa21bd3",
      "d209d1322d004e73a5f1f89a6311b519",
      "972a3a6ded434720a03cdb8875ff8836",
      "bc90defa4b6e40929590afc2d2cf6ffa",
      "bd8d22deb4b948bdb68c45c9d28b9eb7",
      "59bcc2c9208b48098d17443773dd4c98",
      "b97c45b83963405883f6fedbc82234b1",
      "a4462cf4d4ed45879d6bf660389e8acf",
      "a3e43dfe9fc24c92bb9a4b018dac4595"
     ]
    },
    "id": "VkQpUPCTPHJS",
    "outputId": "9e42c8c4-2847-40aa-b443-c124656a902a"
   },
   "outputs": [],
   "source": [
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    processing_class=tokenizer,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "3_JlGtyyPGkJ",
    "outputId": "771b549b-552b-4f20-a3d2-2557d7b7c43d"
   },
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182,
     "referenced_widgets": [
      "abd30b40d7ee40599483b0463b564f5a",
      "e45a25a1c6a24e3f9d17b4b425f17ecb",
      "b0e75120496f468abaed8cd2af4f90cd",
      "02781a89e89e4404b474335f8769d134",
      "0f927a5bd83d48a0b32399b2d1363329",
      "8313ecadf23e4e888a45d9b14c04255e",
      "abf2885f32bd444a96dab12fa01cf9e4",
      "d94a765ad5f74491b905860d51c9ada8",
      "3b49063890a049078d81c78653cabd7b",
      "327b38250eb94f6eb7c124ba0c7781d3",
      "4af08a7039db45c9be42af78fc3d6e65",
      "2c6021df80fe42199d357dfe4ea5c89c",
      "328382cd7ba04a9e9f688373d0357bd9",
      "c6b1af0df9d94a659a5b1c804e097986",
      "dc3f0ba87e384216b160b4c52a81213d",
      "c1b9bda6bab5467c8eb1893bba390f74",
      "5f8f3682edd84ce4b8e3449d8a3003ac",
      "7d14fac37a5a4104ac32bea20588254c",
      "6f4bcaf0a2f34f008060b132a3641fe3",
      "2d5172bbf0f34438b481148908374bbc",
      "44a6bc22ad7d4b5c9b4203e3a0319a75",
      "fce427b723ff47b8a2babdbe98c90127",
      "f515ec58c55241e2b7c77ea2a7aedd18",
      "57a0bb4d16154792862826fb350318bc",
      "55543f913aff4a15bb68aabda3348b0f",
      "f16185f072854933a9ba184398c690ff",
      "8a79c114e5ca43d5886791ef9d36c526",
      "5e9ba94b5e6a4f6ea85da6525159735a",
      "89c3ce2c1fa54551868318d7b00939c9",
      "03ec2ad88a5b4887811d6c8c01858d63",
      "2277b4bf8bf844fcb53c7fa8c5a64458",
      "b984c37f09b74442910cd982bdebd779",
      "96ceb730a8194c69ac9c76193e68fdf7"
     ]
    },
    "id": "mSBJOxzNPiNK",
    "outputId": "da7da0e3-367e-4d75-ce3a-1c9f507dc8f8"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1RwKBbH9qgY"
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGDvsMkw9qen"
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449,
     "referenced_widgets": [
      "cd62609c9bb24d978afdc8edeca0c655",
      "a8643a62644a4c83a2778a3f1ce0f006",
      "4b36a83fcaa648ba908f76eb03065364",
      "ac2ebde3f2354304b1d3a306ef9b8435",
      "0313d53b638348719062544c05b22572",
      "b2930a9dc2cb46809e6ad458680f2da0",
      "9eb9cf38289c4b789f53f3b884f1fb96",
      "ab0537471d604eceb3d4ef846cdf0011",
      "3d52cc07bd2e4f13bc6d8d22699393d9",
      "00d5811eae544c609bf6a4885893a5af",
      "eccb76f31e6543078574e55848506ce5",
      "4242c9e5312f44fe90bf0aded1468a03",
      "6ff8732177dc4df4a3f5c81d223f084f",
      "a94617d08b3542908684d6ee1dc2e37a",
      "ddb1c43f85fb40b39dfc64c01f76b49c",
      "1569f3611b6744ea9083edfc02edfab0",
      "fc0dac03bd85478593cadb9840e9105b",
      "34b4b3e0994d43fe90a0b103fe6e3fa3",
      "5b21ca4b63cd4c85a309c4ec103e99d6",
      "f6ab8f6eca88461a9e5b50c7623abe23",
      "c31b3b9911ec4cfc97297ca5e655e8d4",
      "45e222bc915c41cc8f8a9ec9dbbb6175",
      "1a0efb2e5d5c4157a157fc83f7bbfe8f",
      "8e4e0789f0684edc895423369ac401af",
      "f29d8190f96e4a48996ea1e28f6fc11a",
      "9c76fec66402406c8d850b5ebf876841",
      "9301a2c5ac164242a88c524eacc95178",
      "63ada71f043e4be1b935b80d0ee4b3f4",
      "ca4015d103804cc887873c6dd7a36549",
      "fc619b0eb75b46a58bb513269eae1512",
      "ec50b740b7d24a2d9a772703ba33dacb",
      "27b5dddc41d64e17b7611358150ddba9",
      "d5c9297c07af49d68fed65b9c68d76a5",
      "38f29c4ebc3242c38fac5d2e1b0e89cb",
      "2c342d79ea4f4af59732d365a8166f05",
      "19d72fa23509462e820f792767c3718c",
      "78008d6efb7f484495eb38b1b9874764",
      "992cd53653e24fbaaa50898cc5add304",
      "2dc9832f0ebf4c66846ab2be60bd1fd6",
      "213d982e8c294088b0837ac281b5dc11",
      "07ecee1d2aa846cb981ab44d6ad358b6",
      "81c56f1f00c2438b94ff5079063bcc24",
      "0b6ebb5caf324f3ba67845699916a394",
      "6e3c010a016940308e7a6e35b2a88762",
      "b8b330810a464535beb894f47b3d9622",
      "c14efb1e1a8e4ebb8724a7a5ee338954",
      "0b402984144044a4acd388c2f0ea0d67",
      "53a5ace207354a75a326221fbd8896d8",
      "00c9865a11764a3989f25ac4aa9cfe7e",
      "4fdb5094043448c29c75f5edb02739a9",
      "f8b69e911d044b3db7bdbe93c9ce6f4d",
      "fb47ecce502448359e5f2bca77e8d515",
      "d23191d01d7c41d0b5709662cf57761f",
      "b89962848bc84642b7f54b954bb7e395",
      "43273691eb19435bb4aad521425cbfb7",
      "c7ca1b9ecb9a411090f5b8c2e364603e",
      "c639d6a97fc24d8980f9ba77b0cda461",
      "1b27244ca0a7460b9bd27aefa3edf507",
      "3a612a45c3fe46dc96fd42581f2437a5",
      "31f33ad3be654e39a349ec4d051bba25",
      "46edd459820d46228c3d5c39fd94e6a1",
      "ba2fed157fa44922ba0903e27d5ac861",
      "11daa63af3f048c1a2d6cebaccd340d6",
      "b2e977c4297a47c086ad63e72e06b740",
      "ff84b8c6af00471197e554e015736fe1",
      "2bde0f4893164650aa5779ac0ffbdbf7",
      "c819bbe4b751449e8622659f07a5aab8",
      "5252c41964c342aeab019e468f9be7f7",
      "577fefb6e9cd434989b2cbee0b94d3cc",
      "439ce23f918b4e88b38951057e7fde5c",
      "cc6892dfd6114c8bbecc38c2e12de792",
      "426593a4c2544481befd4bd2b540c5da",
      "969b0c43eed64b69a6cda3577cddcaec",
      "4384d048d4bf438795c67e3dfdee2512",
      "dffa4a4d769a4125be70b73d5b01b343",
      "640ae95af69e44c396d8f58c9684bea3",
      "80ff7da2495d4486a7073d9fa5611b5c",
      "ac0fc947f5a7404eabbd31ba6d5a800b",
      "67ec0ba6e5264500b5c8b95b43bf9ab8",
      "c6005e7d427949d3a9b9f175befdeb6b",
      "78f6c04cabd144dcb9398f4f97c9d370",
      "59f19a670c4d40d889c7bd2ede1e55d9",
      "114eb0168b834ba284f954205db50ef4",
      "85c3a79b20f347e49b9b1f320a34e54b",
      "8ebffbd940344329b27f443b2224a49a",
      "536940fd39584806b642c680f8ffb7be",
      "171ef0d603e343df9447f17f00dcc8d0",
      "634465170e6d44778c86592ee5d92dc5"
     ]
    },
    "id": "f8bu_63E9pSs",
    "outputId": "69fbaefa-22ba-4d15-90fd-c8528498c741"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZxmmpye9a78",
    "outputId": "a92cb369-e124-4820-83b6-bb4a5fab3620"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"Write a Python script to train a Decision Tree Classifier using scikit-learn.\"\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRuVMW_h947E"
   },
   "outputs": [],
   "source": [
    "# prompt: how to push merged model to hub\n",
    "\n",
    "merged_model.push_to_hub(\"SmolLM2-FT-BigCode-Python-Lora-merged\", tags=finetune_tags)\n",
    "tokenizer.push_to_hub(\"SmolLM2-FT-BigCode-Python-Lora\", tags=finetune_tags)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
