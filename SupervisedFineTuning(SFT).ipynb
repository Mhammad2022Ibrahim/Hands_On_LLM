{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvUnqnN2ufxH"
   },
   "source": [
    "# When to Use SFT\n",
    "Before diving into implementation, it‚Äôs important to understand when SFT is the right choice for your project. As a first step, you should consider whether using an existing instruction-tuned model with well-crafted prompts would suffice for your use case. SFT involves significant computational resources and engineering effort, so it should only be pursued when prompting existing models proves insufficient.\n",
    "> Consider SFT only if you: - Need additional performance beyond what prompting can achieve - Have a specific use case where the cost of using a large general-purpose model outweighs the cost of fine-tuning a smaller model - Require specialized output formats or domain-specific knowledge that existing models struggle with\n",
    "\n",
    "If you determine that SFT is necessary, the decision to proceed depends on two primary factors:\n",
    "\n",
    "## Template Control\n",
    "SFT allows precise control over the model‚Äôs output structure. This is particularly valuable when you need the model to:\n",
    "1. Generate responses in a specific chat template format\n",
    "2. Follow strict output schemas\n",
    "3. Maintain consistent styling across responses\n",
    "\n",
    "## Domain Adaptation\n",
    "When working in specialized domains, SFT helps align the model with domain-specific requirements by:\n",
    "1. Teaching domain terminology and concepts\n",
    "2. Enforcing professional standards\n",
    "3. Handling technical queries appropriately\n",
    "4. Following industry-specific guidelines\n",
    "> Before starting SFT, evaluate whether your use case requires: - Precise output formatting - Domain-specific knowledge - Consistent response patterns - Adherence to specific guidelines\n",
    "This evaluation will help determine if SFT is the right approach for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "491a2fe4ee53421f9c49c8da6d869555",
      "6eb3685bd29046d3a47ac6a561f9feca",
      "55e3c261b14e4bc58a16146dbfe55d4c",
      "df7e2604d71e45e38312d0c75b85c14b",
      "c33cf33ea05446058736dfc6743a6591",
      "ec246f1adb374277930c8717732c92c8",
      "74b08bd10f274520b2b71477d6d3427f",
      "7453c56d6a5e4ba8b6cb77c1dd3f8a69",
      "2dd40f9c1812426b833d5c8264bcfdf3",
      "aedcc977fc004d63a6ac96c28a5976be",
      "fed3e6f641264771b2b0243d9f50f5bf",
      "59c2ddd1eb5041498abed66f5f461b02",
      "52471245d620421e824c0d624c680b08",
      "adb3c0f8260e4def8d2a04c79afdfe7f",
      "217f93f817e84f3ebc7d9a3a441883bf",
      "5a3c5874fcc942c08cb6ee1411c5e334",
      "d33f8dfeb00941dcbd6607347a8e45e8",
      "e2d79bde943e4a6fa411aa7817ea3166",
      "9554c4f34fdf4cd1bcaf647ca03ac17f",
      "27341e89b80c4927a2affd43a84edf20"
     ]
    },
    "id": "p0wwy2k0Kg4-",
    "outputId": "734823bb-9b33-4d19-d132-cbffc8e4f40c"
   },
   "outputs": [],
   "source": [
    "# Install the requirements in Google Colab\n",
    "!pip -q install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKuUkOLgwbC8"
   },
   "source": [
    "# Implementation with TRL\n",
    "Now that we understand the key components, let‚Äôs implement the training with proper validation and monitoring. We will use the SFTTrainer class from the Transformers Reinforcement Learning (TRL) library, which is built on top of the transformers library. Here‚Äôs a complete example using the TRL library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81EBQOaxwdMN"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"all\")\n",
    "\n",
    "# Configure model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(\n",
    "    device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "# Setup chat template\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Configure trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ug_f4ifw2Ht"
   },
   "source": [
    "\n",
    "\n",
    "> When using a dataset with a \"messages\" field (like the example above), the SFTTrainer automatically applies the model's chat template, which it retrieves from the hub. This means you don't need any additional configuration to handle chat-style conversations - the trainer will format the messages according to the model's expected template format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kozEPu5TxmBE"
   },
   "source": [
    "# Packing the Dataset\n",
    "The SFTTrainer supports example packing to optimize training efficiency. This feature allows multiple short examples to be packed into the same input sequence, maximizing GPU utilization during training. To enable packing, simply set packing=True in the SFTConfig constructor. When using packed datasets with max_steps, be aware that you may train for more epochs than expected depending on your packing configuration. You can customize how examples are combined using a formatting function - particularly useful when working with datasets that have multiple fields like question-answer pairs. For evaluation datasets, you can disable packing by setting eval_packing=False in the SFTConfig. Here‚Äôs a basic example of customizing the packing configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJbF2bZKxpxk"
   },
   "outputs": [],
   "source": [
    "# Configure packing\n",
    "training_args = SFTConfig(packing=True)\n",
    "\n",
    "trainer = SFTTrainer(model=model, train_dataset=dataset, args=training_args)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrnqOkJrxtod"
   },
   "source": [
    "When packing the dataset with multiple fields, you can define a custom formatting function to combine the fields into a single input sequence. This function should take a list of examples and return a dictionary with the packed input sequence. Here‚Äôs an example of a custom formatting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJwNZbe9xu-s"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "training_args = SFTConfig(packing=True)\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPDVfeUwyzCF"
   },
   "source": [
    "# Metrics to Monitor\n",
    "Effective monitoring involves tracking quantitative metrics, and evaluating qualitative metrics. Available metrics are:\n",
    "\n",
    "* Training loss\n",
    "* Validation loss\n",
    "* Learning rate progression\n",
    "* Gradient norms\n",
    "\n",
    "\n",
    "> Watch for these warning signs during training: 1. Validation loss increasing while training loss decreases (overfitting) 2. No significant improvement in loss values (underfitting) 3. Extremely low loss values (potential memorization) 4. Inconsistent output formatting (template learning issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUiGsaVxzwU0"
   },
   "source": [
    "# Warning Signs to Watch For\n",
    "Several patterns in the loss curves can indicate potential issues. Below we illustrate common warning signs and solutions that we can consider.\n",
    "\n",
    "\n",
    "\n",
    "> If the validation loss decreases at a significantly slower rate than training loss, your model is likely overfitting to the training data. Consider:\n",
    "1. Reducing the training steps\n",
    "2. Increasing the dataset size\n",
    "3. Validating dataset quality and diversity\n",
    "\n",
    "\n",
    "\n",
    "> If the loss doesn‚Äôt show significant improvement, the model might be:\n",
    "1. Learning too slowly (try increasing the learning rate)\n",
    "2. Struggling with the task (check data quality and task complexity)\n",
    "3. Hitting architecture limitations (consider a different model)\n",
    "\n",
    "\n",
    "\n",
    "> Extremely low loss values could suggest memorization rather than learning. This is particularly concerning if:\n",
    "1. The model performs poorly on new, similar examples\n",
    "2. The outputs lack diversity\n",
    "3. The responses are too similar to training examples\n",
    "\n",
    "Monitor both the loss values and the model's actual outputs during training. Sometimes the loss can look good while the model develops unwanted behaviors. Regular qualitative evaluation of the model's responses helps catch issues that metrics alone might miss.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL2WRNEa1UAW"
   },
   "source": [
    "# Evaluation after SFT\n",
    "\n",
    "After completing SFT, consider these follow-up actions:\n",
    "\n",
    "1. Evaluate the model thoroughly on held-out test data\n",
    "2. Validate template adherence across various inputs\n",
    "3. Test domain-specific knowledge retention\n",
    "4. Monitor real-world performance metrics\n",
    "> Document your training process, including: - Dataset characteristics - Training parameters - Performance metrics - Known limitations This documentation will be valuable for future model iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja7YXtcL6q3F"
   },
   "source": [
    "# TRL - Transformer Reinforcement Learning\n",
    "TRL is a full stack library where we provide a set of tools to train transformer language models with methods like Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), Reward Modeling, and more. The library is integrated with ü§ó transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMIqs6rJKg4z"
   },
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p>\n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `HuggingFaceTB/smoltalk` dataset</p>\n",
    "    <p>üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n",
    "    <p>ü¶Å Select a dataset that relates to a real world use case your interested in</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oa_EeZQiKg5C"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE9oM3zcKg5E"
   },
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoJPHQ1CKg5F",
    "outputId": "db2c580f-db59-45fe-9c28-1af1a2ae3027"
   },
   "outputs": [],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqSsAz-HKg5G"
   },
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "ad9b4c498f4447d28ccd0ee2fdb7cadf",
      "917e09a301d04457a93a25157e490b72",
      "fafd6f72721341f7b54bec3daa40e5cf",
      "e980a6c752904e4bac3e8723cebbc1f8",
      "66445bc783884f78a90b43d7e43cb45f",
      "57e8489cbde54049803d59bd3d444e88",
      "3992c2ef4745490ab0b782afccd18bbc",
      "ba4fba38c28746d0b8d5f5862dfdddcd",
      "d5f38e1e5c9643dc9adbfdf8b8f6e9cf",
      "fdc1ebfe589544dd824cf8f2aed56578",
      "7a5733c665114e7c93ee191bed357968",
      "88e64a6f4650402a96e6231755c4dc5b",
      "bf1c34ed7e9d4eb18da85cae179b4dbd",
      "fce15d696f5546a694d1aa6cbc13bb5d",
      "4e6a497664a949fdb2a44407367e61a1",
      "e844d3aa5f0a4dee9ad0db494681cd52",
      "7ef8ad5cdfb746e8a5172cc11c76725b",
      "19dfb40a6a1b46da9aae3846c10d8a24",
      "86fce8c457fc436ba210086603dc1fd6",
      "8727bfa269a8499187294cc8e5c6dc67",
      "de1aa34271e04bd4be38f15b1066df95",
      "41cd2265849842c7b02065d9f5c000ed",
      "7283b33bf3ff41ef85009b43dbdb9564",
      "203687e9007e4329b8cf4106b3eb1d0c",
      "8756ba2af7764d5d9971413b2e0927b5",
      "2438d85d1966419c8d9de68e70cfe7fe",
      "7238fb82a0154bafad592b3bb483a7c9",
      "fed11d17bbb643c284a3fdffb09228ba",
      "717ecb883b3a400cb4b1df2c0ddaa43c",
      "3d581fc8b31148c59758a7265294a131",
      "a161473c731f4bdfb5ffe34b16411644",
      "20e208bcd6bf4bcabe84488ddfb533bd",
      "cc05ad655b004efdb7cadf9348402ac6",
      "af76d79d49564d928a41f92c8551ee0a",
      "5532372f6b7c435f86cec388682bb53c",
      "17dc2ea5ccb14c449d4f3f119b174f1f",
      "1e32659ccfd5431abcfe2111e2ba9779",
      "1c73f19bc5e64aa08a1030a3523910ca",
      "1154317fcf6845188124139048539a38",
      "7891605dfaf44111926a0503ac5306ab",
      "ebfa10bdb0214271ae91515f56411ffd",
      "6e2f48e691e448c88480ad11e49e64f1",
      "6d8841655a174fc8b7b2613f52038bef",
      "9bc0b159560f4a73a49d0d0fb7cbbb1a",
      "308b174af25e4d6f948a97d729362fb1",
      "7378a26acefa4c41b2eb1316a153c680",
      "9f2cfa0cdd3f4be2a78f55d5dfc6e360",
      "f787e32f299b4133847343f2bc601d43",
      "943b4369f25a4ec8ac888745d96bf89a",
      "41048c94816743b1b037b7adaa557475",
      "1f6459ea64f24b6ca3f7e321ff1f4544",
      "a691e137503a471c8fc628374b48b328",
      "e2362cf5ab384503b84c44bb51cc086a",
      "3bdf27a5a70f47fcb6ee977b77f8fe6e",
      "789bc8a3093c4cf69c503e34dbed4add"
     ]
    },
    "id": "O03hR6TyKg5I",
    "outputId": "991b0ac7-d161-4ed3-a97a-0e5dd15bfe5c"
   },
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUD8hcx5Kg5K"
   },
   "outputs": [],
   "source": [
    "# TODO: ü¶Å If your dataset is not in a format that TRL can convert to the chat template, you will need to process it. Refer to the [module](../chat_templates.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQLBZfWwKg5M"
   },
   "source": [
    "## Configuring the SFTTrainer\n",
    "\n",
    "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "f4f4906cce0d4f5dbd924e71ada46613",
      "d0f5f256cb8a48a78ec32b139e72e1cf",
      "bd60607fcdb0452f83a189e360e4168b",
      "6dcc14c95dff4295afeff83e3d2ce8a6",
      "12667ac4cff54a249421718b1789a09c",
      "a8692e00a97549959cd27ed73d4cd76c",
      "b7b188fbf0e94b5c81e042e34d9f7b86",
      "6067e155fdb5463aa7fc2c9e6ff52752",
      "135a85e9dccf4bb89b8243e43d0f9988",
      "8061bf5fc101497e95349f42ac122dd5",
      "979dc71db2354ac19108f5bcaba8c00e",
      "371d433625854dcfadfe7fab69908201",
      "c3be89a963144034a026e8922c1ba3a4",
      "03f682613d48473aab602ca114ffc716",
      "eb84ffe169a940cebef122ca05012b61",
      "b52a651792e84a3db256691de266f428",
      "636207b8fc614292ae88044339bf03e9",
      "d2a3905c53fd45789d5e06a70db17d7e",
      "96949e3407c34023981d522e65d1b7e4",
      "32ac9820aeed477692af1412a242527c",
      "160919ec68ba4b8ca6e87bc58a7c9d3d",
      "92bc5f7f8b064abcb3b69a52eedcabcd",
      "f12e15a5062241199c6164eda975a4e3",
      "0a86a41e817443338f4b7a8999753f83",
      "9cd85ad390ea480db252605664316412",
      "23a66afc539149b89383bbdddf520d14",
      "544b703317bd42f4b42509cd80f3363e",
      "aac70c5c2f30443e9ff88b1d141f70c7",
      "de4be4cf926843bd87b909a2803ec7f2",
      "b88f21b8b26b4d8b842765f5825e9627",
      "fe9378afe360430d843fd8faffde465c",
      "1ca5f1d9c8d341ecbc2f331fee7c8338",
      "c60029a7f84f4a28a42a21f0a95b4db9",
      "4f1d91adb90c4fa287acd45d0da2ff5c",
      "471f4202e01b4d2fa5a557dc2b2b953b",
      "4b199c4d1f924d499c91fb46e4c19737",
      "9d55bc82e505486f8bb1fdde7e6632fc",
      "34e43e0b949c4361a3f6972c149a72eb",
      "44ba6c85b11a420183d8f28888a213ea",
      "6f7b2ebe7f57423a824636d3e6a12115",
      "4f600ada318e4df7a73ffca2af170f7e",
      "61d000de0e2a4a2ba2b00a03875eecc7",
      "95f2ea64fa8b4ca0ab4501eff8a7c257",
      "49335d539e6f4fd59e9a3f51fcf39010"
     ]
    },
    "id": "f2S9qsXIKg5O",
    "outputId": "d8fbd00e-05e6-4a89-e02f-d86e07f9df12"
   },
   "outputs": [],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    # tokenizer=tokenizer,\n",
    "    processing_class=tokenizer,\n",
    "    eval_dataset=ds[\"test\"],\n",
    ")\n",
    "\n",
    "# TODO: ü¶Å üêï align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwhMz_plKg5Q"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6VZT08imKg5R",
    "outputId": "1128a570-612d-442c-ed67-c08df614665e"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "afd418c7434941ecb30b7fdb96b22853",
      "f5f83e3d67db46b8a8716210fdb172fd",
      "e507bf2966664e0abfa31293a02ae2d9",
      "211bc028369b49b6880a29e0775594d9",
      "af02b79b21564a8c961ba5766bb66f19",
      "9268950388ac41ea8fa3cd0b87635291",
      "553239830e5148d59d2d7e55cadfca74",
      "6d916252b7f147468da7dd9ed3a5eaef",
      "9d70d55d105e4c1bbe8ef28b648eaa8f",
      "03d634603424446a9c620c0e3c236089",
      "aa99c7bb74c9413fb61102283d705e7f",
      "3673be6308904654a0dad92f212fe036",
      "43a7c9d9bce3429d9b83288035bc16e3",
      "a01422cdfd824045929518b24f5d3ae5",
      "1d804f579285486d96b1709ea67d1e67",
      "acc0fbae650a456881dea8c3e9924584",
      "848bef04b09e41b5b13384e7d9ea06d9",
      "267927455a104bd9af47eb60690f1e25",
      "478fdc12f08640d49cf18acddd340a97",
      "74a15c60237e410aa2ecbdd86c1583c8",
      "96ccc01e4fb64426ac24c03428745e28",
      "25f3eb77d79c4d128b7e02dd11d5ad45",
      "c98463425a73402b92ad2649b5d150ce",
      "fb3cb2cb042e4f88845d04f0c1de5618",
      "21e3ab8c23354726833a493645df26c5",
      "b660df0dc1b949648542ef12ca020ccb",
      "065c313798de4389ae6ded5b25cd052f",
      "b6c991a3f0ae4611b97eefc331399f63",
      "8b1a9e9b18374bdf92d405d772c49108",
      "63ba272688bd4abf868d296a5d6146dc",
      "44caeae92c3045f7b1f2203ff190726c",
      "e07ee067944448309275a5661590ff65",
      "ab244690ba2342e19141d1f2a10956c3",
      "a33ddc27a49f4d4884f27d1c9fd4435f",
      "c8f3523ecd714eb8b54b0394a2d8ccd0",
      "15dba9480b6e4597b45051f1276611db",
      "ddeb5122cec0493c9beda0f590d55832",
      "e072e392d8f04550b5136d98050cc8d2",
      "e69eadde8f594c6e87f0a68e9863f102",
      "b4a03b3fb53943c0ba4c951544211c3a",
      "64fead47494e4922ba58d7e192e5c7b9",
      "932c1fddc66e44e3b6f2af366c91f2ae",
      "fd630b00f82b4234a18c5945e3b1a716",
      "d168b0c8142e4d2a82082d500784602f"
     ]
    },
    "id": "I_rooDiHKg5S",
    "outputId": "a633b420-11be-493c-bf5c-8e97898e79a5"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mIk_0RvKg5T"
   },
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687,
     "referenced_widgets": [
      "9683b6d1aa064a6caed090b8cbd81910",
      "40ba147d4ce848ee81ca88296b2e2a55",
      "ef74fd7be3f24eb58418e3f2f610b3d4",
      "4a18f6bc3d0c403086d3b77959af082f",
      "78d9c0f8e910439e876b906abb184e1b",
      "42760c72fb024ee192c4239a40155aec",
      "5a41445bd5c5436f8d0368153171a355",
      "37cf296649ac4125a4569de7e8b356a4",
      "a1d63a136f0d4ba2953e65b5e0b541bc",
      "7e4559cbf73e4d1cbdd50f76859f057b",
      "29c04a021e2a45ee8e56be4a9f8cc455",
      "f3ce6869b9b145db957fbf954c3ea9b2",
      "ca776eccfb4f4fa3b97117a3d41b3d4b",
      "0af7bf30444a46969d22f95bdb9144c3",
      "0f7eb5c104f84a9dad351759a46e4fdc",
      "015a76b36f684665a7e7986c7782e5d2",
      "8bbf5a2e18504ed3bb1b61b6e2e6f481",
      "f37a0cf3cd2840d082378fb4ebf7de8e",
      "efb98e268d2841768cdb36ba271b9688",
      "a241cdf3871a43ee8b16f3840492ec60",
      "cf18ec79109f4b0693659df7c9e017d4",
      "a0b180ad8327401f96108d367c3b7f5e",
      "be223f8fb30f4ab4ab9b2fa758b3fae2",
      "d738b7e11b1848f789ef548ba1438ff1",
      "0a397fa843c640f08f6ea880afdd10c5",
      "bb23609c93424e0081bc07f4fe66f3b3",
      "6705a6866e9b486eaabbb1c8e81a7cb5",
      "673b3ffe94cb4a72848ef5a947fe9936",
      "f1482d79a8ed4cb49fb9468e1cd2ceab",
      "5f28daf4849f43e3893d8e06cfef509e",
      "8578f49a671a4adf9d32abb901ff8fb6",
      "33fc8bb7bbbe46e5a52e9b15f096e579",
      "e9f7502906dd4ee19f78a46df9e7507d",
      "4cb9c7755d1646d8b72a2f95eb67c78b",
      "8a506f2cb92047b2958304b35070dc98",
      "95dc045768da4f51adcb86f658a6d24c",
      "f301ba8e1d5b486f91cda0abc573a994",
      "d0bf038c76474e92a9f3d930558b2639",
      "0428c14bd61849f99166812c06728c1a",
      "95e8ff6777fc48e1b8911e2b40855612",
      "9d22205782a944ddba5a7df3ceb5039a",
      "9811c24b8fa04ce9a5d7ce49693bc703",
      "7fb0f9a8b00a44398b33a4770a8ab6f1",
      "6cbf3c754efb489ba5e841b9e957ae24",
      "7dcac99823d4469eb111c5cbdbfa1bd7",
      "e04808fd838a454c98f5adfe91673ae7",
      "797603c826de45df914f059dfbe14e2a",
      "851a4fcf73b54eb28ce208aba06100d9",
      "fcd8e0680f0a4b2a999cc18638c3c001",
      "00dcec153a11472caeee7d44ff7eb5d2",
      "99d49a7f48a942f6a98801a40bbead6b",
      "3ffbf633ba9845418a54a971f0c9a35d",
      "56605dd7d7ea493a81b997007f32613e",
      "e96c41d1ab464c0a8902c6b247a21609",
      "5b5c360d516848b2b0c9c5eb61221751",
      "5afa5fbe3e024c72a10966103aac2f62",
      "f158a4d1ed6e4b46ab120c953c1bca16",
      "b1c75f4a4e0a4af2afa7f32a510f2f99",
      "044869fbfcc34edf9be235c108598251",
      "b3132ebafbcb40aba479eee31be02c37",
      "ef9e76623bf448a5ae638a611a64d97e",
      "0b9f3b46545f4120a54730cc7cf72c61",
      "87933ea96e104823ad59fbb20271e6ec",
      "180f527c305242c78339c5ee07b7edd8",
      "a0c3087988344a6d939e5fb981ada4ec",
      "30d10088049e468ebf0022a2f6ef4eaf",
      "4da45aeed8eb40ac9feb714ccc25e0e6",
      "ab27e65c9ccd472fbb97e8e54a6da60e",
      "beb6496654c8431ebb94a84b4dbaf2fc",
      "f02f258fecf34fb38aea93a47207d3b1",
      "de513ecb262348c6a26eff1b6454387e",
      "aa5606475bd74aa88825552734cf94f8",
      "e6220d3dfe2d42d7a2c82a291215fdaa",
      "d9e0e7e9c7444757a31237e56976a88f",
      "1657a93434874cb79f445a379453ed72",
      "5af49408a4ce439c9e846d32f3985fb4",
      "2d4f5a7ab4a64fcfaf96fe333333f592",
      "9f3cd6569d8446b08bb154184a1fb8a0",
      "38a5373b4bf04c62a276316355a4de0e",
      "f25a33d86511418eabcb581ca0826c55",
      "212387f795ec4201b990275cbede7480",
      "471079b61218451a9d6c5be56037c05b",
      "044931fab08f46ffb350ba492a8a1c64",
      "a036c5e2a2ef4eb2b49cad1e75af2745",
      "185d091d8d9843e8ac6f0d544c0f6846",
      "f8132275fcce4d2b81a4704dc849d3f9",
      "c55b799b4e4744328358c4bce7882e41",
      "baedf5db28ef489083bdd6dba73a9196",
      "f5f2c6a4c99841c8ade46b36558f036a",
      "77e1081e699e46578a5dc3cc27273781",
      "055a43c647ef49bbaa072be97446d13e",
      "dc705133c66e4a61adb6cbc7d9a89a39",
      "04894d1d5e564a3fa40d42fddf165d30",
      "1f453735ef444284a65c1459fd48d698",
      "9db6f875aea54fe081f801653d152e59",
      "3dead278976746529c7bd70b53c11430",
      "eb51e747d2f749f29bced74d6de250bd",
      "30d99dd5ea8b46aaac32cdd52139e65f",
      "6681e2ebd2bf47d8828cfe3dd1c4f575"
     ]
    },
    "id": "n0p9bEvYKg5T",
    "outputId": "1df2e714-4d1c-4f62-e372-3945b82f614f"
   },
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# TODO: use the fine-tuned to model generate a response, just like with the base example.\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"Mhammad2023/SmolLM2-FT-MyDataset\"\n",
    "model_fine = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer_fine = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "# model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "outputs = model_fine.generate(**inputs, max_new_tokens=100)\n",
    "print(\"After training:\")\n",
    "print(tokenizer_fine.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12CDwB2xKg5V"
   },
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyOjQUeYyiBW"
   },
   "source": [
    "# üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "7a84198945a94a92b1adb85ffdc0f59b",
      "6c17231365b84712b17f71d4cfb456fa",
      "f22541a8247c4437abcff04fbc6972da",
      "1bd65f246041492f81af82bac2ee1b30",
      "b42517b4225f4b52b98c22bb81076a81",
      "49f159fad98a4876a15621e76f02f95d",
      "5f47a04463834c959dc94760aa1d6c02",
      "3aa68c447e304cf8965d6fc6c5d3e5a7",
      "b0667524897544f69a43373a6c63e700",
      "6fb5118df2754c0196054758a11637b4",
      "824c5b5931e443628f290151a1e7a37e",
      "1dee8006d0314748aa2006d7e34f1b6f",
      "d022e4401ae54be494cd79dc0536f43c",
      "8722d4643d0140a58ccc138818fc4b6f",
      "44280516fddb49f484faaa7de38c3b87",
      "4d9fe2e797de47f6b455960be13e5362",
      "b34970f935df4d85b71069a9c7539a09",
      "c3911fb4e98d43ccbae3ed7706227640",
      "251f3b1431194d4c95ca961b98cbf973",
      "0e0e399617a44837816dccd50e2c9875",
      "6fb72646a7604f189ddbbf34d267fadf",
      "78f38a5a6fe2426ea2fbeab5ed4d7f77",
      "550d77e4b1204da99862e91f1da648ad",
      "31608477009244cd8fa469a0dc3d1801",
      "9a7c8b1472a249458321356c15209d2c",
      "3e042deb146844f695a9c585bf679667",
      "e6aa76f6e8b64aaabc0c2f4a3007e2f2",
      "8a1afedfb8354475ab21df96c38e341e",
      "ae29e0a1b9ec4b758a04f7851feb3df0",
      "c23014685e23422fbef4dfeaa609c171",
      "36c00d2c847a4644a6543c502e9924a3",
      "581046462bce42f3b817b7fbc44794d5",
      "5c0d6a45c03b49ab81f0e2de80854ae7",
      "8bafcbb89f0640d6b4160bff3ec8de2a",
      "2566dc3949c140f4aaa1d5fda4eb630f",
      "ae4af51c376d43da9e6db865cbda9774",
      "e989c3c52ab94c58ae4b82f4ec8772ac",
      "d520c8c09ff3473dbd854d62bf339d42",
      "a2a5c21b28cd4e41b06760ac3f818e2f",
      "01771273794149d28fd7fbfa31999aeb",
      "2dbea2014c0c4aec91c3ec1113931aaf",
      "0c3083ed1c574e6db5b32d2e110a0e51",
      "7bf1b483e0214cfa892b7a10f50d0c62",
      "77525581525e4da48ddeefae822a4bb6",
      "53af69d3892b48dc897442b00a2f26c9",
      "e1ae9267d34d4199b30550a4f3cff498",
      "7768eeaa24034afaa964310311270b59",
      "12c8093f623649709dd3b4d1a1cae470",
      "4ba6da7458244e378d60efcbb3ee37a9",
      "f9f70368dc3f425f9ae5e9e1cd92c82b",
      "8e8e632f37ae469f97f4149db4acd3c7",
      "62533e79822f4f9b99fb40a438cffa08",
      "333745e0d2f34a52accc0c66c29405b9",
      "5d314208763647da9f2a8ea725cd4491",
      "5edd1e8c4de4416d955b146a9fb724d8",
      "3e9de114d312416699e3634741a673ca",
      "58564e43789b4c87affd52c161f81e32",
      "ec213f74d6744fe09fa7c4bbdd552d7f",
      "00631f972af44fdfb3f9d658004c04b6",
      "73601128da6a4bea95ba5ab0ad1b04f2",
      "86d4cd1ac327469cba0685e4acca855e",
      "aae5dc3b491c427498ded805431382bf",
      "50d4b8beb72d4ba9bd4bd2dc9ce5b0b2",
      "e844e4983ed743999c7bfb3829ac9a22",
      "d84feacda940490597d9209a57845655",
      "075fef5bbe8244249f2ef62de281b8f4",
      "e0301e90c4604b82a70f869d21b6ca08",
      "98adbde97f074328a34ad1a31d2d6464",
      "4fb34d6177624d8ab4460a397e5e0ebc",
      "078cfbf1f5ab41d9a0bb6d97b7211a4f",
      "b283425f1ebb4fff9b2675ae57b148e4",
      "97f50566ea834cd8b370bc871e6a06d3",
      "9d81f165a6fd40d09ea7a6b9ab7479d6",
      "df6bc4a449114cfc90ac02bcd4204407",
      "44566ef8451b48b487e4ad8c341a5f24",
      "ea650a6dd3c34965855a681d9634c560",
      "b084e200b89d45dca379a3172bb557f5",
      "231b287aedbd46be91222a3cd9ee7f59",
      "da3ec20c65224ea8b861fbbb2d325e6d",
      "61523cd64e794c289b76ff20391ba2f2",
      "3916bf7ff2bd45c4bfbc337374ff9767",
      "3f0f672253324c60bbcc89e93f71d301",
      "997768af06e04bceba8f0af8fcbcdc62",
      "57d87771e7354ed1b703bce3bf2f751e",
      "d19a0b76970a4a5881b1bb6f364c7948",
      "bf0597f8c8a1412eb7557aa2cbac4619",
      "e8a332fcd6d14d01aec98fb65061dfab",
      "b221fa21751e40a89ccff4801d664128"
     ]
    },
    "id": "fCzv1oAEydkU",
    "outputId": "22b9bc75-e63a-41aa-ebb0-c92e3dd073e4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set up chat format (important for TRL-style fine-tuning)\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckSvlXpqzhDe"
   },
   "outputs": [],
   "source": [
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-BigCode-Python\"\n",
    "finetune_tags = [\"smol-course\", \"module_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6N10Mv6i4B6A",
    "outputId": "107fb6cd-8ee5-404c-99b9-3f970c8d03ca"
   },
   "outputs": [],
   "source": [
    "# Load the dataset (Python subset only)\n",
    "ds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCejU40cOf-Y",
    "outputId": "16f9fd12-1090-4e70-f090-46f6402ed884"
   },
   "outputs": [],
   "source": [
    "ds = ds[\"train\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "3496511dc7b04c468bdbddde7dd097b4",
      "22296900c4544048a26bb337360408bd",
      "26aeee3c789f4c429f2d62661ef54e16",
      "aa7a8e57cdd44dac92d14a09ce883728",
      "f1c010c3137d4c1c970e89caf54544d8",
      "697688b58e234d2b9e26396156fab6f5",
      "41771b2db0a84b29a8e088b425950987",
      "2f86d50d10d342d590983f365ee4d2a6",
      "1f973159c1664b09941b4ff4e2b55320",
      "19cf0e47c96a47919e58f0fdd77cea8e",
      "c36e69335337412a8c115189c0118d8e"
     ]
    },
    "id": "sufxZaAxa_mZ",
    "outputId": "4ff726ca-9085-489a-c2d1-510d1a38fb5b"
   },
   "outputs": [],
   "source": [
    "def convert_to_chat(example):\n",
    "    parts = example[\"repository_name\"].split(\"/\")\n",
    "    repo_name = parts[1] if len(parts) == 2 else parts[0]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Generate code for `{repo_name}/{example['path']}`.\"\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": example[\"content\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds = ds.map(convert_to_chat)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "6tPpNVsybrOG",
    "outputId": "0fa02528-28f4-4558-a4dc-6dfd4f08ed53"
   },
   "outputs": [],
   "source": [
    "ds[5]['repository_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "qqYpbksDbxTe",
    "outputId": "60dc0a5e-9c45-45fd-f4c6-b1625a7ea125"
   },
   "outputs": [],
   "source": [
    "ds[5]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgrIhazDbJgw",
    "outputId": "5366b8eb-2a34-41da-e312-434780a4ff1b"
   },
   "outputs": [],
   "source": [
    "ds[5]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "85fadb477df3493990df49224a2f3c55",
      "579f4a813aca4affb72d4c40e58c7ba6",
      "922499cb0ff94fb1ab20fcf0b7025d81",
      "c6930e6e768b4995bae19de7a519ae9d",
      "839f530830734c9a989b7814874809f1",
      "a7e98a8f81284a1ab0fea5eca2fe7bb0",
      "73106d99101e44aaad03e2b23f4fccd8",
      "700ec1de19004096b9989845565e0310",
      "64a1f898ec07491aa1967c52e06c320b",
      "108769d0965b447cadb58b2f527a41b3",
      "bce7a38203d9494bbd5e3cea7ccad006"
     ]
    },
    "id": "8rdsg2_ZyxIO",
    "outputId": "d514fb5a-0114-4bcb-8fa1-2e579dca9c41"
   },
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Preprocess dataset: use only the `content` column\n",
    "# now it's messages\n",
    "def preprocess(example):\n",
    "    return {\"messages\": example[\"messages\"]}\n",
    "\n",
    "ds = ds.map(preprocess, remove_columns=ds.column_names)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsM28yoK4e02",
    "outputId": "0ea74ca7-88c7-4437-df1d-91766bc3cd24"
   },
   "outputs": [],
   "source": [
    "ds[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fp9bxVTV6NGP",
    "outputId": "d7117a37-0506-40b9-db4a-a7baeef30c34"
   },
   "outputs": [],
   "source": [
    "# Shuffle and split into train/test manually\n",
    "split = ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "ds = {\n",
    "    \"train\": split[\"train\"],\n",
    "    \"test\": split[\"test\"]\n",
    "}\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94EUW1xmEftX"
   },
   "outputs": [],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    # max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    # per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n",
    "    # learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    max_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=50,\n",
    "    max_length=512,\n",
    "    per_device_train_batch_size=2,     # ‚úÖ LOWERED for GPU RAM\n",
    "    gradient_accumulation_steps=4,     # ‚úÖ Accumulate gradients for effective batch size of 8\n",
    "    save_total_limit=2,               # ‚úÖ Prevent disk from filling up\n",
    "    fp16=True,                        # ‚úÖ Enable mixed precision if on CUDA\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8CDXsCEFDBd"
   },
   "source": [
    "**Use gradient_accumulation_steps**\n",
    "Simulates larger batch size:\n",
    "\n",
    "**batch_size** = per_device_train_batch_size √ó gradient_accumulation_steps\n",
    "\n",
    "Example: 2 √ó 4 = 8 effective batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5RlHXm9MsNNt",
    "outputId": "9ff2913b-ec36-4b3f-dcb7-7d10e88aa8d4"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182,
     "referenced_widgets": [
      "86b69c416ab2435f876eae07562e9c75",
      "5977fcbf3ad64f28b2d71752f36d618e",
      "4f2e9e8d752e4dbfa4b213210fd3bc71",
      "7d77e35ef11a4d9596cb0c5c0f0e3c83",
      "830b878bc03b41c8bc706f9cd8f8edf0",
      "2a4bcf2a7b8b412c82c03c004588a9d1",
      "21660f8fde7a4653b6a678965a775aa3",
      "450249cd4cb947aaa7a3aa32a7b1e07b",
      "0850161daece4b7db4404c44e45f4b9a",
      "131ddfb002554f5990d3be2b64f5d40f",
      "ada71cee80fc4c6ea8984f38e2fd66a7",
      "9e2627be3d6d439381eabc60f2ebb770",
      "a50f6d88d2244264958c67540d343376",
      "a3f436845a1d4d5d8bf70dd98055c070",
      "9d2b58fbac2741529866151bb4e99bd2",
      "fef0672a2e6249da98aaf098d282a689",
      "4b30782f5c4a497cb63c24815c600ddd",
      "98b7785c8edc4d5d9a94c6b9be39fd95",
      "2b0852ac4b5b478f9cfaba36393631c1",
      "79aea1b16af9412c905781d3698ae637",
      "8589abbd71714b3bb7ebeefa94da7043",
      "08012fa012374feb8a5463f60e63bf20",
      "d6ca8be7f41e48ae988cf76f3bc818e7",
      "07d43e7258b8476c8a06c0c83b2aa05d",
      "a1d1b13a83134a8181f077436afa52c0",
      "571705388ce842d09d61cb90662c0f69",
      "a349ea2bfeee4314b47b5b670ff4c689",
      "91a11ac7d0e04e09a6039869b2c5b81a",
      "156a02be4eef464fa8ac2d94e2378d5a",
      "d478ff8421e642ef99a0eadee771c66d",
      "99af6beebdbe4e479e4988355cf5dd27",
      "15e0380d10b84086bba7fbd1fa842be7",
      "0d335e8493f643ab9e633a2a7152aefb",
      "d46d492343db46d4ade59e65494cc063",
      "dd427e519b854a699f3a53fb92f6f5b7",
      "cb60363529ff43abbb50907bf4050dfe",
      "8beb0dc4791f4033877784df52e1284d",
      "20b80cedc72b43d2bbe797e541a299b6",
      "e113a22eebaa41e2925829600fef021f",
      "b5af09ab87374777bb54027f37edf53e",
      "1380921ed96c4305abdb720b88e21fac",
      "c809eab6098542da9de9f36a57fe5965",
      "a3ca0eb1241b4149855e94283f9e4706",
      "f5fc6917caf146ffa4af9d1011337e2b"
     ]
    },
    "id": "ArlYx7j5QqVu",
    "outputId": "6e3a57e5-8910-440e-9b27-ddcf79df8e21"
   },
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "ClDDvsxO5CGd",
    "outputId": "1f17270f-3e80-4da6-d7a5-30bd8a648103"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "aaa519e53cf84570a6d47bf9844828c7",
      "02944d1d0ab84066b30433aa740db16d",
      "5493ea49cd954ba48fd6c2fc64cd30a4",
      "c3ecdd0f8be64ebe83128744ddcf16a7",
      "8e8adf7a8d444e5f90d19f3b47140952",
      "5430cc6a908c4264acdeb65c496fc864",
      "11a2b772a6454c48a23d98bc48bfca2a",
      "72bb95d060d94e2d8a536e3b9aea5d3f",
      "4b352e4b9bf448f3913a2590b88f442d",
      "480bc7663c4f4995a954ac977516872d",
      "11c2b4d683cd43d0adf5ba401615fec7",
      "2c5ff7dbbd624904bfc8602ec9db0a36",
      "68281a8e4def4ab3b903c7bc1e6a1fde",
      "3d1eb81275a24b7cb5fc7e78db6f9a92",
      "89c4b534893a4346a5481bb2eb0c73f0",
      "de259f66342749bbb8b228e1bce480bb",
      "8385b73b428a44b6b42a229ea21ecd6a",
      "d02f059e7cdf49a49e3db721925ee840",
      "addbc3d2ccd84249b862701384b1b668",
      "f1823fdf7bb643fa9987bca1162d6a73",
      "2651035352204ad5b239416cf82d3a4d",
      "e3cba9b9641f490db0876f93ef0c05c1",
      "e20d9340d5d44075988c2bac75a9b96a",
      "e538719ce251475b9c2de7a588354b6e",
      "7397196487a940db830c633da85a3871",
      "c0e396bcf5c24dbe8453824cf45b28e1",
      "35f72fdd6e7c4643af274b5d97ddc562",
      "ce36209285414ceda32da559a9bf7347",
      "784e14e0e1854479b56f29a5d83f6e31",
      "f7e7be3bf7904cb0ba20c89590045f9a",
      "f497d5ada25f44c4b405d11e922d6f30",
      "441602ca68894912adb0783dd3ee368f",
      "db018718e6c344cda16ec1d1bce865a3",
      "4de68dd1676a4da0a7614f7670fbf407",
      "a7aee6d075a6424c91aaefc158b031b5",
      "1e60c51b5b0544c3b6e166f602080857",
      "78d384d568a648b5a9ba357e383f593d",
      "25b706dd2d3b4dd2965aa5ffd87c602d",
      "c85d1f32133d4240938c4cce4071c032",
      "e969a60d968f4b0f8e56dd5e952aba90",
      "4465ab9851d940f28f45dfb992f5f55d",
      "5a9b2aa970404bd79e13849c8f4ad39f",
      "62c12adc0d8c4176af4239908cecf191",
      "bf166fecfc6543bb831bd17d5ddf3ab5"
     ]
    },
    "id": "VqRSQMd8uUhX",
    "outputId": "7bb783da-6a58-45b5-d04b-c7dda6338059"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305,
     "referenced_widgets": [
      "16dfdcda623b47f2a19221e0512556b5",
      "cc90078ff2e24436830d728334e0406d",
      "187831fec856458cb8ac083e964bd15a",
      "73051f46c93f42ca8e79d5fbc131a7bc",
      "248c114a68be466482940f0a4a7622be",
      "d9d11fff38ba4bcda214256a98014578",
      "3e7eea22471c4dca900a5dc413907c69",
      "fa250dbfc23944e0bd9cf54b16b6127c",
      "fef2ae58683c43dd83474490c547a6c8",
      "8a456d17a0f14d7780739383c63df2cc",
      "5c34a4f657754e9cab8bb5f3d41fcac0",
      "c662abca6b7741fbbb38172c15af50c4",
      "57755fb848a0470f8c426549e58610f4",
      "d91f15cf6482484a8a53ef132abb7d68",
      "6c09036d8fe84afcadbecc6cbd337ddb",
      "a5bc169de4344a659969d2d78a35b37c",
      "209df4a58b3d4a7ab976e8204787faa7",
      "05b76ede856f479cb34605fc3c88db40",
      "7ee2738dc05848168e31a12a625cd1a4",
      "dd8f2c085f5644dead25a42be62f0e28",
      "cde657026a1e4480833db246d51d179a",
      "d0c536a775a643a5bcdf24a78802296d",
      "56ade7d4ce3c4b139c4cce25210d1211",
      "3e84f5d614b94c52a83623e7a00dccb9",
      "e1c26cdbcd1a44c6ac3244e0536d2b98",
      "b8ed88fe73eb4a38bab84262bef5f718",
      "6855c69c6b5d400399475fd02514e41d",
      "3b9d9c2530924751b78b8bb2937546af",
      "5e6ab9c31bfa496a94ccecf908fb1bc8",
      "e74240fa4b8d4b9692886b83ec35a791",
      "bcdc77e0c8b7422fbae9f46f09bd5a95",
      "72a1277d9c414434a4e6e5f832d15c9f",
      "8e4cb14b79b14b2fab97fcb1d374b694",
      "0f2245fa8cbc405f841853a4e4f6bfcd",
      "bd0f6bfa4e1449a59116c586ce38df49",
      "660974686a5a4fbb9c09e79cc2daf845",
      "1f9d73a43c924056bc7632d8148ea779",
      "5c2851a8bf274c56b287508f7bb97420",
      "6934869d63cd448cbe5b2d4a39ea3676",
      "01619967f2c5444ca069964ae2d99577",
      "278612dea0ee400bb2e8f717253e0fc6",
      "e089582f9b85484eb6de8026c47cbd8e",
      "b9857356c45f4e27b7f64a75d4b9d5b4",
      "d44f674edcaa49cb8c43f6ee2fa98df4",
      "332c78d3e336473aa470f4fe765ba08a",
      "80970a19aaad43aa9fd94bd1a4d73f43",
      "17bc5818d2474ed6b6645799a049fcec",
      "9e4f5e82c6af4c07b1126f610cdcb40e",
      "30277887fb504e2998cb5622b4162136",
      "f4057f376ea242caab37fe3d2bc9e2b4",
      "d3ac1296d29d4d99921e28a55330f443",
      "97eb937e28944c8a8c3fcf90b1002197",
      "7f8a489638784781b7bc1b49ca23c2e6",
      "74825b1496b140bfb106b42acecf4eee",
      "96c93f5b72804638b028a1d3adb56eed",
      "0a3e8d267f644bdea5c1e810a44e9961",
      "4392668c893f4a7096e5ff2e8daf7851",
      "33df59170f0346429a168bf40ba673ab",
      "e2ed023efa4747b38ff31acc1c6b9817",
      "80f13bd79359428da001b3071979bb43",
      "987632f00f9b41bf9a47640fb02bb4b5",
      "67ba73c960754961beb689595e72ffd3",
      "5f9b58c76fa947eda07137b0b1fe0005",
      "0a9f9a702cfd41aba9ea76e9f7fedcbc",
      "8da703c707c64736bb5869a7453d1c0a",
      "42a5426427b44789ad2bb04db956dc15",
      "61a8828122974ec6a31a1e720f8b98d4",
      "b7459c97ff734c559503f8bacd3a1bfc",
      "92cddd7496b049078de4c49e976f81cf",
      "b25cbe54ad1a42c0babb8de9e75d8b76",
      "c09f6e3614d54bca97ade686fccfa35c",
      "a871bc78349240e3a7352e1087d2323b",
      "809107c685264a14abc1e1910b392b77",
      "9daa14a05ecd467f851ffd4a0fa3be65",
      "dc5c7916ee3d4d99a2b3a6f043867893",
      "6a37361903614a5883a7ad0de89b84ae",
      "e90a93827e224544a603cd0d3e3bf316",
      "6d200b763cf54a18aa8bd6454bd85858",
      "4190b4dd8a774a32bac86c940c3e9e59",
      "87382621f82b4572934e3b7f1712bb08",
      "87678f5f030f4adcb7a09cc73e4df238",
      "f8d78efb425a48da87f941dee2e6cc57",
      "8fd8f4bd1c1d4071ba04763768d2b7d9",
      "3c4a7aed43b8430ea2231b1677e471d7",
      "a479a923d04b4885bf45f80153ec9ba4",
      "6c4be2fef02a4825bbcf07faa1ddd99e",
      "3e41ee98b5ab4ec5989d540e37b4c1de",
      "77fda3e22b7f41fb9b6648c26aa9ba10",
      "692da35e1c754e76bcb76854729ed9fe",
      "3bce045391e04d2782e4e76d16a7a5e4",
      "9b0452b139d44c90acd4e5ec596e48e3",
      "f404a5dfd1c04ed7b7e53044295b2c06",
      "2905f107f6c0402db6cdbc3546a83920",
      "28e116bd14774fb7b794cc7642ca5869",
      "603ccbaecd7a4a5a9d929720ebb3a13c",
      "738417c4287043bdb75495964dd36df9",
      "6b06d0a134a04224b01390a0e683ad07",
      "a6873bd3da504793b6a57aa6ce72825d",
      "72deb3aae35943929c712982a577be1b"
     ]
    },
    "id": "fSfQ93vuodIW",
    "outputId": "d3423b21-94fe-4f47-bf9d-018383fe3767"
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"Mhammad2023/SmolLM2-FT-BigCode-Python\"\n",
    "model_fine = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer_fine = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZrJ7m0xoMDg",
    "outputId": "276e93d7-0d54-4222-c56e-fcea5cee6506"
   },
   "outputs": [],
   "source": [
    "prompt = \"Generate code for `machinelearning/decisiontree.py`.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_text = tokenizer_fine.apply_chat_template(messages, tokenize=False)\n",
    "input_ids = tokenizer_fine(input_text, return_tensors=\"pt\").input_ids.to(model_fine.device)\n",
    "\n",
    "outputs = model_fine.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(tokenizer_fine.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIMZ19e6o8R3",
    "outputId": "0359c8ec-dd71-459c-cece-82416b903e23"
   },
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Write a Python script to train a Decision Tree Classifier using scikit-learn.\"\n",
    ")\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_text = tokenizer_fine.apply_chat_template(messages, tokenize=False)\n",
    "input_ids = tokenizer_fine(input_text, return_tensors=\"pt\").input_ids.to(model_fine.device)\n",
    "\n",
    "eos_token_id = tokenizer_fine.eos_token_id\n",
    "outputs = model_fine.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "output_text = tokenizer_fine.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZXQdWnovF1U"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
