{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38FDR0NfJumw"
   },
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmEMuVqGJnXQ"
   },
   "outputs": [],
   "source": [
    "# !pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off\n",
    "# !pip install -qqq flash-attn --no-build-isolation --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0oYEAKiXVFg",
    "outputId": "0e741dfa-213d-4c47-d3d8-869526d9f6a3"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq datasets transformers trl peft accelerate bitsandbytes wandb --progress-bar off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hogwsrVkJ0YA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJNL8n8yKBgI"
   },
   "source": [
    "# Import and log in to Weights & Biases\n",
    "Weights & Biases is a tool for logging and monitoring your experiments. We’ll use it to log our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "8HjgKlMjJ966",
    "outputId": "a6ec0e5f-d2aa-4e8a-bb4d-a3a4268e19dd"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWNwBakrLDUX"
   },
   "source": [
    "# Load the dataset\n",
    "Now, let’s load the dataset. In this case, we’ll use the **mlabonne/smoltldr **dataset, which contains a list of short stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "0d556248da714dcf86c13abeacd43a33",
      "d2e25ab6bbe54cb6af336f53d20ce8a4",
      "1a757a00344340df92e3d3421238ad68",
      "f0d2f2fc14984bb6a9fade68155210ba",
      "43d5a41fa3384d88b979ddb01dcafe21",
      "7371d0c5e85942f390ba198642a41f4b",
      "f50976b779b44c50a483bb4ffb359c6d",
      "c15fe3a9035d441b9f245b916a5fd179",
      "4aefb0ece3f641808c8b4b16311460e4",
      "1ce7575ac56b4a6da425f5fe4a1fa1cf",
      "8bb0d696ad484ed199fcc634612a7b16",
      "80c1b4f688994292bfb6c31227d970cd",
      "25a337844712432895b9dafe5fd5948a",
      "5d2c8de2b180472fbf831a5a5aa04b45",
      "332b3e8456b04f0a95e62463b49b7f5a",
      "6d88c496c3f54069868d25a791622a0c",
      "90dc8b75f97e40328427ad82e854bd57",
      "593d4fca38b94311abea2842de036ca2",
      "6082ed04cd85481799901366ecc90fa6",
      "4927b3a7b5944bda9f6d483cfea6ce3c",
      "8f24bad7faef42dab1366f2ef7e90cf9",
      "4e7c2c7d5c31456f899a4dbf6aac8ba0",
      "5ddc1c8851524636a0769c76e3a6c1ef",
      "bd7045ac35544f8fbac692e09234e139",
      "802e9d83a8814ef19a69608b4699a283",
      "131d19a47290414fb210503e5e168e5d",
      "6d42742eefeb4cbc92e24687ca11e5cc",
      "07229c1e74314209a8e16f7f3d57e1ec",
      "9aa66bab70a34aa8b1b3d06d58237977",
      "317fe99f38c745959bdffda475f59820",
      "96848d8219dc49dcb9e38091c6321830",
      "00a45f804a5a4bf9a5e2e4d16953c506",
      "c49812fb0ef34d9c85902193a5ae0b6a",
      "52f9aa8e4d8a4bc498264a0e84331623",
      "2fce86dac3cf42bea8233c94298aac5d",
      "e649355aaf704b618d9d520fdc8fea0b",
      "55c93aa7d61e43f6ad17f74df120ad0d",
      "6651131618124dbbae2bcfe1fc9b44ac",
      "a40fd1f50c4543f989602273da2011a4",
      "7e6b65dc47ce44ada744196855c514ae",
      "526119a6f2ae4e10a5dedf079284f808",
      "6fd043df315e4ed1a04b55482cf29a8f",
      "a6d4cdeb486947d98fa433b9aa1ae079",
      "8cba7c93608949499a49f74f7bc3c2d7",
      "7ff3c3b2e06546a9b30d436d4d284641",
      "87f30af747824eb7b87cbaf1f4783a6c",
      "2f477e5ae15548eaa6f91aec6dc30065",
      "c4574ba23ec147eaad0acfbdc2628898",
      "2ecdf62c17c8430a8be458507475a918",
      "0a41c085c3dd491d86fd75d93da177f0",
      "db4d43fab83d4835bed1493d6d7033a6",
      "c970ffac2f3f4c3e9d657fee5390779a",
      "c1f69b5991d547ac9750fcdf4ad761ba",
      "01d4ddb0ec504c0ea08581f8bbeb7ac9",
      "779a4cd0b91e4178aa276a7c046f3909",
      "ca7af6f4736045ab887582909dcde9d2",
      "ffa3387cc2ff40179435b2b47d124ee2",
      "bc6e02daca274ddc842d684e5c2c4f1b",
      "0dac7f9621bb421885f88765cb811649",
      "910bebe96748497c8c291c056a1ad38a",
      "1783e9efdbab44e48d9df3ecf6d9522b",
      "c806da4e4b0545a499ed02e54ece633c",
      "f8f6e57537344793a398dfae2294fc55",
      "b32f3fc7c5ac4e22980870bc208acb44",
      "c36eda54ba3543c697ca5abc4ee0c631",
      "a1e64cb3489246d4b2e2cef4eb334539",
      "227883c2160742eeb1fa223f271a04c3",
      "a37c07a8520e405aa797db32d1e86d11",
      "c8463e8e99e442729979fe52e7fbb31c",
      "33cc5ef3bc184038b2c7c83f415b619b",
      "a1bf8fc294ff4b018fc015351add7224",
      "dfd68b95e6a24fb78cfdcf40eb6063cc",
      "c0b8a0007c5a400d83507a19673068d8",
      "253ec5a771514c49b346d911ef446a06",
      "61e04622a60049b4b7bb131a47f6d839",
      "0eeddf232c7143f38c561e1ef26a6c16",
      "393dbac67fc5446a9c50f7d2c8392290"
     ]
    },
    "id": "bJUB48FFKhei",
    "outputId": "8f08973a-96f6-42e8-dc4f-1186f0aac24e"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mlabonne/smoltldr\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lcx8IVc3LIYJ",
    "outputId": "2d6524fc-c7d1-4171-bdb6-3a168b170f24"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beRuMMvRLfCX"
   },
   "source": [
    "# Load model\n",
    "Now, let’s load the model.\n",
    "\n",
    "For this exercise, we’ll use the **SmolLM2-135M** model.\n",
    "\n",
    "This is a small 135M parameter model that runs on limited hardware. This makes the model ideal for learning, but it’s not the most powerful model out there. If you have access to more powerful hardware, you can try to fine-tune a larger model like **SmolLM2-1.7B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o--PXRibLSwi"
   },
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSxd7TMBZ0RJ"
   },
   "source": [
    "# Load LoRA\n",
    "Now, let’s load the LoRA configuration. We’ll take advantage of LoRA to reduce the number of trainable parameters, and in turn the memory footprint we need to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9sPVpJ8Ll2i",
    "outputId": "8d3da68a-eaaf-4334-b302-a04e7e67df0b"
   },
   "outputs": [],
   "source": [
    "# Load LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeCZOIptaEuh"
   },
   "source": [
    "# Define the reward function\n",
    "GRPO can use any reward function to improve the model. In this case, we’ll use a simple reward function that encourages the model to generate text that is 50 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f33d9d7e"
   },
   "outputs": [],
   "source": [
    "# Reward function\n",
    "ideal_length = 50\n",
    "\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    return [-abs(ideal_length - len(completion)) for completion in completions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-is20A7bahGR"
   },
   "source": [
    "# Define the training arguments\n",
    "Now, let’s define the training arguments. We’ll use the GRPOConfig class to define the training arguments in a typical transformers style.\n",
    "\n",
    "If this is the first time you’re defining training arguments, you can check the TrainingArguments class for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riLs6CmJaM4E"
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"GRPO\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=96,\n",
    "    num_generations=8,\n",
    "    optim=\"adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZpj4honavzJ"
   },
   "source": [
    "Now, we can initialize the trainer with model, dataset, and training arguments and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UokaVqm8aoa9",
    "outputId": "27f616a4-616d-44c0-81ae-22c17b26b26a"
   },
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[reward_len],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "wandb.init(project=\"GRPO\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A46WFcBecMey"
   },
   "source": [
    "# Save and publish the model\n",
    "Let’s share the model with the community!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "1e5c4d17ec154e29b85046853c000273",
      "5f934201155148eea60cda7275ecba65",
      "2487eb81de9a493b8fba281fcae7891a",
      "8ef3be4d40ca4740bc6226497c2deb40",
      "41a195c605be4e388dc39eec693f71b8",
      "39d7e6b394974bd4830330c2c05195dc",
      "fc3b64c3175648ecb4a3dd7eed9e5bc4",
      "0ef0cabf854441cc920633b3621b2e6e",
      "3847af0b7c5b43d198a6851c5df7327c",
      "18850fe7149047d2a6c875825eb50a67",
      "32300d929fea4181941c74450c6731fb"
     ]
    },
    "id": "msvnfTU_azCN",
    "outputId": "f0fdf997-5db1-4026-97ea-9b74301d9632"
   },
   "outputs": [],
   "source": [
    "merged_model = trainer.model.merge_and_unload()\n",
    "merged_model.push_to_hub(\n",
    "    \"SmolGRPO-135M\", private=False, tags=[\"GRPO\", \"Reasoning-Course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4hiExuzcabp"
   },
   "source": [
    "# Generate text\n",
    "🎉 You’ve successfully fine-tuned a model with GRPO! Now, let’s generate some text with the model.\n",
    "\n",
    "First, we’ll define a really long document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7TAHL4dcgbJ"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "# A long document about the Cat\n",
    "\n",
    "The cat (Felis catus), also referred to as the domestic cat or house cat, is a small\n",
    "domesticated carnivorous mammal. It is the only domesticated species of the family Felidae.\n",
    "Advances in archaeology and genetics have shown that the domestication of the cat occurred\n",
    "in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges\n",
    "freely as a feral cat avoiding human contact. It is valued by humans for companionship and\n",
    "its ability to kill vermin. Its retractable claws are adapted to killing small prey species\n",
    "such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth,\n",
    "and its night vision and sense of smell are well developed. It is a social species,\n",
    "but a solitary hunter and a crepuscular predator. Cat communication includes\n",
    "vocalizations—including meowing, purring, trilling, hissing, growling, and grunting—as\n",
    "well as body language. It can hear sounds too faint or too high in frequency for human ears,\n",
    "such as those made by small mammals. It secretes and perceives pheromones.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8f9ba6a"
   },
   "outputs": [],
   "source": [
    "# !rm -rf ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "d45f1526814c430cb6f83a5257551cdb",
      "57f2627823f945cf9e703bea3e3d3eb8",
      "02c820f9e7d6406cabc3ec4be6b10f57",
      "247962f5f0224f3299b8df2f29ba4206",
      "6b9703ca6b48447895f493eb6dfc4b99",
      "9be573292c7142bdadb15b128efaf6c8",
      "8d977c9c11db4e4fa543cb91c42d3dd9",
      "34b8035809ec4e1689cb34c828e1d0b5",
      "38442a0ba60a49af92e1ca9def064228",
      "e853e609fef04e439a93f4ee68a3fa0a",
      "0d02216136bd4995b0e3112f9f3c3f10",
      "91246ca6b9f74471beb8710999f913d4",
      "dfe84aafa56f4ef49c128a23df22dc3a",
      "21b8a4c9c934451c9cbc4d72088c7328",
      "154b1e2cda6d43c49fd57ecf736e3100",
      "0564f849e50548be87b1c0ecbabc21be",
      "bb708a1797764cb7974610281dad1712",
      "79d27de47540481785e0ece48699a1e8",
      "e00f80eab160459da4332ac994dc06fb",
      "0a9262284daa4a738f97875a5dfb0727",
      "009a65973e3d46c18e53cbd8e956ea0f",
      "d1161b9810004fa78f230ff0f4326603",
      "c43284074b4348b4858abf8257396dea",
      "d81c9a57ace24028aa64f5516bfcfac0",
      "cb0784538b8d4a89b2b385937b1e73ac",
      "4e6be9b02dca40a0bf111e328d99c9d4",
      "233a585e21144d72b66cf1ea98d36ce4",
      "9242e209f5d2401da7d110a17028b543",
      "804cbba0adf94b769ec4430542d77826",
      "1da86c801702486b856e6670436e6e42",
      "cbf88c11c4264160b62fe4f0c01866b7",
      "e3716a15cf744711996475839566b36f",
      "269f8f76a0a042619b1d65afcaa35ac7",
      "559b4df0738d423b90799c307964c3eb",
      "b420fcfe6c38499eabb4e023bc57b8ab",
      "b4666449b68e4568b171c59cc1ed5780",
      "6320233d6aba466094dceb04843db545",
      "6c1ccb1d840b49fd93ec6f53733169ae",
      "cd9d39d0d9bb4e8db791dad75344c2b1",
      "2c4d748a873d488991c3ecb1a1da97fa",
      "bb20677237d14b53adfa6991faae27cb",
      "351012593cc848f99410465abac1df0b",
      "c07afef44c3d47f8914f7157ce64a405",
      "8a99fe6f0d5e40049dfcb77591b51367",
      "aeda03f93fbc45e2b43fa5c607828080",
      "c2c3be91b1a74efd8c7ca66ba14a9f97",
      "653dd4175c974b7094479e13631ed223",
      "001b0dbaac0c424e99e2c941e74feb2e",
      "819c78b11378482b84df6da966226f58",
      "8e51474c70e343f292c7af35d631cd07",
      "9fa7fbce23774189a68340876307f270",
      "68d3de0a5a424396994e6d0f5740fbff",
      "0cf5f74bd24b4f99bfc6943a3dbfd089",
      "8dcf59fec258453bbb921b8818368dc5",
      "a3b0f364e05a4651a2d0ca88b61c79b5"
     ]
    },
    "id": "gXLTxVGS5OQC",
    "outputId": "0b5551be-a629-4933-fecb-5171047c31f6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_id_tok = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "model_id = \"Mhammad2023/SmolGRPO-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmSRCgPy5PrT",
    "outputId": "b68f6668-64df-4940-d4fe-08f98568287b"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.5,\n",
    "    \"min_p\": 0.1,\n",
    "}\n",
    "\n",
    "generated_text = generator(messages, **generate_kwargs)\n",
    "generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Ttz1290UE0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
